---
title: Triton Survey
author: tfruan
date: 2024-04-11 20:00:00 +0800
categories: [Triton]
tags: [Triton, Survey]
---

# background

triton æ˜¯ **ä½¿ç”¨ Python æ¥å†™ cuda** + **blockå†…çš„ä¼˜åŒ–äº¤ç»™ compiler**

## numba

å¯¹æ¯”å’Œtritonç±»ä¼¼çš„è§£å†³æ–¹æ¡ˆ [numba](https://numba.pydata.org/)

```python
BLOCK = 512

# This is a GPU kernel in Numba.
@jit
def add(X, Y, Z, N):
   # In Numba/CUDA, each kernel
   # instance itself uses an SIMT execution
   # threadId å¾ˆé‡è¦
   tid = threadIdx.x
   bid = blockIdx.x
   # scalar index
   idx = bid * BLOCK + tid
   if id < N:
     # There is no pointer in Numba.
     # Z,X,Y are dense tensors
     Z[idx] = X[idx] + Y[idx]

...
grid = (ceil_div(N, BLOCK),)
block = (BLOCK,)
add[grid, block](x, y, z, x.shape[0])
```

numbaç‰¹ç‚¹ï¼š

- æ˜¾å¼æ§åˆ¶ launch è¡Œä¸º(grid, block)ï¼Œå’Œ cuda å‡ ä¹ä¸€è‡´
- ç¼–ç¨‹èŒƒå¼
  - è¾“å…¥ä¸º tensor
  - SIMT è¡Œä¸ºï¼Œæ§åˆ¶æ¯ä¸ª thread çš„è¡Œä¸º
- å’Œ cuda å¼ºè€¦åˆï¼Œæ”¯æŒå¤š backend å›°éš¾

## triton

```python
BLOCK = 512

# This is a GPU kernel in Triton.
@jit
def add(X, Y, Z, N):
   # æ“ä½œå’Œ threadId æ— å…³
   pid = program_id(0)
   # block of indices
   idx = pid * BLOCK + arange(BLOCK)
   mask = idx < N
   # Triton ä½¿ç”¨æŒ‡é’ˆ
   x = load(X + idx, mask=mask)
   y = load(Y + idx, mask=mask)
   store(Z + idx, x + y, mask=mask)


...
grid = (ceil_div(N, BLOCK),)
# no thread-block
add[grid](x, y, z, x.shape[0])
```

triton ç‰¹ç‚¹ï¼š

- åªå®šä¹‰ grid ä¿¡æ¯ï¼Œ thread blockå†…çš„è¡Œä¸ºç”±ç®—æ³•å®šä¹‰
- CTA å†…çš„ wrap æ•°é‡å¯é€šè¿‡ tuning config é…ç½®
- ç®—æ³•èŒƒå¼
  - è¾“å…¥æ•°æ®ä¸€èˆ¬ä¸ºæ ‡é‡å’Œ tensoræŒ‡é’ˆ
  - SPMD
  - å±è”½å†…å­˜å±‚çº§ç®¡ç†

## cuda vs triton

cudaå’Œtritonç¼–ç¨‹æ¨¡å¼å¯¹æ¯”

![cuda_vs_triton](/assets/img/blog/img_triton_survey/cuda_vs_triton.png)

ç›¸æ¯” cudaï¼Œ triton å…¶å®æ˜¯ **æ€§èƒ½å’Œæ•ˆç‡çš„ trade-off**ã€‚å¾ˆå¤š cuda ä¸Šäººä¸ºçš„æ“ä½œè¡Œä¸ºéƒ½æ˜¯äº¤ç»™ compiler æ¥è‡ªåŠ¨å®Œæˆï¼Œå¾ˆé€‚åˆå¯¹ç¡¬ä»¶äº†è§£è¾ƒå°‘çš„åŒå­¦æ¥å®ç°æ–°çš„æ¨¡å‹ç®—æ³•ã€‚

æ¯”CUDAçš„SIMTç¼–ç¨‹èŒƒå¼ï¼Œç”±å¤šä¸ªthreadå¹¶è¡Œå¤„ç†ï¼Œtritonæ›´æ¥è¿‘SIMDç¼–ç¨‹èŒƒå¼ï¼Œä¸€æ¬¡å¤„ç†ä¸€ç‰‡æ•°æ®ï¼ˆåŸºäºblockç®—æ³•çš„ç¼–ç¨‹èŒƒå¼ï¼‰

ç›´æ¥å¯¹çº¿ç¨‹å—è¿›è¡Œç¼–ç¨‹ï¼Œæ¯ä¸€ä¸ªæ“ä½œéƒ½æ˜¯åº”ç”¨åœ¨å—ä¸Šï¼Œä¸å†æ§åˆ¶å•ä¸ªçš„çº¿ç¨‹ï¼Œçœå»çº¿ç¨‹ä¹‹é—´çš„åŒæ­¥ç­‰æ“ä½œ

![cuda_triton](/assets/img/blog/img_triton_survey/cuda_triton.png)

## æ¨èrepo

OpenAI [Triton](https://github.com/triton-lang/triton/tree/main) æ˜¯ä»€ä¹ˆï¼Ÿè¿™ä¸ªé—®é¢˜å¾ˆå¤šå¤§ä½¬éƒ½å·²ç»å›ç­”è¿‡äº†ï¼Œé˜…è¯»å®Œä»¥ä¸‹ blog ç›¸ä¿¡å¤§å®¶ä¼šæœ‰ä¸ªåŸºç¡€çš„ç†è§£

- æ¨å†›è€å¸ˆçš„ [è°ˆè°ˆå¯¹OpenAI Tritonçš„ä¸€äº›ç†è§£](https://zhuanlan.zhihu.com/p/613244988)ï¼Œå¸®åŠ©å¤§å®¶å»ºç«‹ä¸€ä¸ªå®è§‚å°è±¡
- [OpenAI/Triton MLIR è¿ç§»å·¥ä½œç®€ä»‹](https://superjomn.github.io/posts/triton-mlir-publish/)ï¼Œå¯¹ triton çš„æ¶æ„æœ‰ä¸ªå®è§‚ä»‹ç»ã€‚
- è‘£é‘«å¤§ä½¬çš„ [å¦‚ä½•å…¥é—¨ OpenAI Triton ç¼–ç¨‹?](https://www.zhihu.com/question/622685131/answer/3217107882)ï¼Œå¸®åŠ©äº†è§£æ›´å¤šå…³äºè¯­æ³•ä¸Šçš„ä¿¡æ¯
- BobHuangå¤§ä½¬çš„ [æµ…æ Triton æ‰§è¡Œæµç¨‹](https://zhuanlan.zhihu.com/p/712640431)ï¼Œå¸®åŠ©åˆå­¦è€…å¤§è‡´æ˜ç™½ä¸€æ®µ python ç¨‹åºå¦‚ä½•è¿›å…¥ triton pipelineï¼Œç„¶åè·‘èµ·æ¥ã€‚
- ç†è§£tritonè¯­æ³•çš„repoï¼š[triton-puzzles](https://github.com/srush/Triton-Puzzles)
- å¾ˆå¤šç”¨tritonå®ç°çš„kernelçš„repoï¼š[lightllm](https://github.com/ModelTC/lightllm)

# triton å¼€å‘æµç¨‹

op define + launch function + call

ä»£ç æ¥æºï¼š [tutorials/01-vector-add.py](https://github.com/triton-lang/triton/blob/main/python/tutorials/01-vector-add.py)

- op define

ä½¿ç”¨ `@triton.jit` ä¿®é¥°çš„ kernel ç®—æ³•

```python
@triton.jit
def add_kernel(
    x_ptr,  # *Pointer* to first input vector
    y_ptr,  # *Pointer* to second input vector
    ...
```

- launch function

```python
def add(x: torch.Tensor, y: torch.Tensor):
    output = torch.empty_like(x)
    assert x.is_cuda and y.is_cuda and output.is_cuda
    n_elements = output.numel()
    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)
    add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=1024)
    return output
```

- call

```python
torch.manual_seed(0)
size = 98432
x = torch.rand(size, device='cuda')
y = torch.rand(size, device='cuda')
output_triton = add(x, y)
```

# triton ç»„æˆ

triton çš„[ç»„æˆ](https://github.com/triton-lang/triton/tree/main/python/triton)ï¼š

- [language](https://github.com/triton-lang/triton/tree/main/python/triton/language)ï¼šå‰ç«¯çš„triton-langè¯­æ³•
- [compiler](https://github.com/triton-lang/triton/tree/main/python/triton/compiler)ï¼škernelç¼–è¯‘çš„è¡Œä¸ºï¼Œä¼šæ ¹æ®åç«¯è°ƒç”¨å¯¹åº”çš„pipelineï¼ŒæŠŠtriton-langä¸‹é™åˆ°ç¡¬ä»¶æ±‡ç¼–
- [runtime](https://github.com/triton-lang/triton/tree/main/python/triton/runtime)ï¼šcacheï¼Œautotunerï¼Œ jit ç­‰ç»„ä»¶
- [backends](https://github.com/triton-lang/triton/tree/main/python/triton/backends): ç¼–è¯‘å®Œåï¼Œåœ¨æ‰§è¡Œæ—¶æ‰ç”¨çš„æ˜¯çœŸå®åç«¯ï¼Œä¾‹å¦‚ [nvgpu](https://github.com/triton-lang/triton/tree/main/third_party/nvidia/backend)ï¼Œè¿™é‡Œä¸»è¦åŒ…å« compiler æ—¶çš„ pipeline ç»„ç»‡ï¼Œ launch å‡½æ•°(ä½¿ç”¨æ¨¡ç‰ˆåŠ çœŸå®kernelå‚æ•°ç”ŸæˆçœŸå®çš„launchå‡½æ•°)ç­‰

## languague

ä½œä¸º triton é¡¹ç›®çš„ fontendï¼ŒåŒ…å«æ¯ä¸ªåŸè¯­çš„æ„é€ è¡Œä¸ºï¼ˆä¾‹å¦‚opçš„æŸäº›ç±»å‹ä¼šå·å·è½¬ä¸ºé»˜è®¤çš„f32æ¥è®¡ç®—ï¼‰ã€‚

å®˜æ–¹æä¾›äº†åŸè¯­çš„ç”¨æ³•æ‰‹å†Œï¼š<https://triton-lang.org/main/index.html>

**tlè§„å®šäº†irå¿…é¡»è¡¨è¾¾ä¸ºstaticçš„**ï¼Œä¸å…è®¸ mlir ä¸­ dynamic shape çš„è¡¨è¾¾ã€‚

## compiler

ä»¥ nvgpu ä¸ºä¾‹ï¼Œtriton-lang çš„ä¸‹é™æµç¨‹ï¼š

triton-lang -> triton dialect -> triton gpu dialect -> nvgpu dialect -> llvm ir + nvvm ir -> ptx -> cnbin

![triton_arch_now](/assets/img/blog/img_triton_survey/triton_arch_now.png)

triton-lang(python) -> ast -> ttirï¼šéå†æ•´ä¸ªè¯­æ³•æ ‘ï¼Œé€šè¿‡code_generatorç›¸å…³ä»£ç ï¼Œå®šä¹‰visit_Assignç­‰å‡½æ•°ï¼Œé€šè¿‡mlir builderç”Ÿæˆå¯¹åº”çš„mlirã€‚

compileræ”¯æŒå¤šåç«¯çš„æ–¹å‘ï¼šé€šè¿‡Linalg dialect

![triton_arch](/assets/img/blog/img_triton_survey/triton_arch.png)

## runtime

### jit

`@triton.jit`è£…é¥°å™¨ è¡¨ç¤ºä¸‹é¢è¿™æ®µä»£ç æ˜¯ä¸€ä¸ª triton kernel

ç¼–è¯‘æµç¨‹ä¸­ï¼Œé¦–å…ˆä¼šå°† `jit` ä¿®é¥°çš„ kernel ç»™æ‰“åŒ…æˆä¸€ä¸ª `JITFunction`ï¼Œè§ [runtime/jit.py](https://github.com/triton-lang/triton/blob/main/python/triton/runtime/jit.py#L824)ã€‚

```python
def jit(
    fn: Optional[T] = None,
    *,
    version=None,
    repr: Optional[Callable] = None,
    launch_metadata: Optional[Callable] = None,
    do_not_specialize: Optional[Iterable[int]] = None,
    do_not_specialize_on_alignment: Optional[Iterable[int]] = None,
    debug: Optional[bool] = None,
    noinline: Optional[bool] = None,
) -> Union[JITFunction[T], Callable[[T], JITFunction[T]]]:
    def decorator(fn: T) -> JITFunction[T]:
        assert callable(fn)
        if os.getenv("TRITON_INTERPRET", "0") == "1":
            from .interpreter import InterpretedFunction
            return InterpretedFunction(fn) # ä½¿ç”¨Interpreteræ‰§è¡Œï¼Œå®Œå…¨ä¸ä¼šèµ°åˆ°ä»»ä½•ç¼–è¯‘æµç¨‹ï¼Œä½¿ç”¨numpy&torch api å°è£…
        else:
            return JITFunction(
                fn,
                version=version,
                do_not_specialize=do_not_specialize,
                debug=debug,
                noinline=noinline,
                repr=repr,
                launch_metadata=launch_metadata,
            ) # ç¼–è¯‘
```

> `@triton.jit` çš„ `do_not_specialize` å‚æ•°ï¼Œæ¥é˜»æ­¢ triton ç”Ÿæˆè¿‡å¤šçš„ kernelã€‚
>
> triton jit ä¼šä»¥æ¯ä¸€ä¸ªéæŒ‡é’ˆå‚æ•°ä¸ºå‡†ï¼Œå»ç”Ÿæˆä¸€ä¸ªkernelï¼Œæ¯”å¦‚æŸä¸€ä¸ªå‚æ•°è¿è¡Œæ—¶å–å€¼å¯èƒ½ä¸º1æˆ–0ï¼Œé‚£ä¹ˆ triton å°±ä¼šä¸ºå®ƒä»¬å„ç”Ÿæˆä¸€ä¸ªã€‚

`JITFunction` å¯¹è±¡ç»§æ‰¿è‡ª `KernelInterface`ï¼Œå¹¶å°è£…äº†ä¸€äº›è°ƒç”¨è¯­æ³•ç³–ã€‚

```python
# python/triton/runtime/jit.py
class KernelInterface(Generic[T]):
    run: T

    def __getitem__(self, grid) -> T:
        """
        A JIT function is launched with: fn[grid](*args, **kwargs).
        Hence JITFunction.__getitem__ returns a callable proxy that
        memorizes the grid.
        """
        return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)

class JITFunction(KernelInterface[T]):
    cache_hook = None
    ...
    def run(self, *args, grid, warmup, **kwargs):
        ...
        key = ''.join(sig_and_spec) + str((constexpr_vals, excess_kwargs))
        kernel = self.cache[device].get(key, None)

        if kernel is None:
            # Kernel is not cached; we have to compile.
            ...
            src = self.ASTSource(self, signature, constants, configs[0]) # ast è½¬ ttir
            kernel = self.compile( # ttir ä¸€è·¯ä¸‹é™åˆ° cnbin
                src,
                target=target,
                options=options.__dict__,
            )
            ...
        if not warmup:
            ...
            kernel.run(...) # è°ƒç”¨ drive apiæ‰§è¡Œcnbin
```

`lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)`ï¼šå°†gridå‚æ•°å°è£…ä¼ å…¥è¿›å», é™¤äº†å®šä¹‰çš„kernelå‚æ•°å¤–ï¼Œè¿˜ä¼šé¢å¤–ä¼ å…¥num_wrapsï¼Œ stages, streamç­‰å‚æ•°ã€‚

```python
# äººä¸ºå†™å‡ºçš„kernelè°ƒç”¨
add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=1024)

# çœŸå®çš„kernelè°ƒç”¨
add_kernel(x.data_ptr, y.data_ptr, output, n_elements, BLOCK_SIZE=1024, grid, num_warps=4, num_stages=3, extern_libs=None, stream=None, warmup=False)
```

### autotune

@[autotune](https://triton-lang.org/main/python-api/generated/triton.autotune.html) ï¼šç”± `@triton.jit`è£…é¥°çš„kernelå¯ä»¥è°ƒç”¨ `@autotune` detectorè§¦å‘è‡ªåŠ¨è°ƒä¼˜

ä½¿ç”¨ä¸Šéœ€è¦æä¾›ä¸€ä¸ªconfigsï¼ˆåŒ…å«åœ¨kernelä¸­å®šä¹‰çš„ `tl.constexpr`ï¼‰åˆ—è¡¨ï¼Œautotuneä¼šå¤šæ¬¡è¿è¡Œkernelå‡½æ•°æ¥è¯„ä¼°configsä¸­çš„æ‰€æœ‰é…ç½®ã€‚ï¼ˆé…ç½®æ˜¯äººä¸ºç»™å‡ºçš„ï¼Œæ‰€ä»¥ç©ºé—´ä¸å¤§ï¼Œä¾èµ–äººä¸ºç»éªŒï¼‰

- keyï¼šå‚æ•°åˆ—è¡¨ï¼Œå½“keyä¸­çš„å‚æ•°æ”¹å˜æ—¶ï¼Œéœ€è¦é‡æ–°è¯„ä¼°tuning
- prune_configs_byï¼šç”¨æˆ·å¯ä»¥ä¼ å…¥å‡½æ•°æ¥å¸®åŠ©å‡æï¼ˆä¾‹å¦‚åŸºäºæ€§èƒ½æ¨¡å‹çš„å‡½æ•°ï¼‰ï¼ŒåŠ å¿«æ”¶æ•›
- reset_to_zeroï¼šè¾“å…¥å‚æ•°ååˆ—è¡¨ï¼Œè¿™äº›å‚æ•°çš„å€¼åœ¨è¯„ä¼°ä»»ä½•é…ç½®ä¹‹å‰å°†è¢«é‡ç½®ä¸ºé›¶ã€‚
- restore_valueï¼šè¾“å…¥å‚æ•°ååˆ—è¡¨ï¼Œè¿™äº›å‚æ•°çš„å€¼åœ¨è¯„ä¼°ä»»ä½•é…ç½®ä¹‹å‰å°†è¢«é‡ç½®ä¸ºåˆå§‹å€¼ã€‚ï¼ˆæŸäº›kernelæ˜¯inplaceçš„ï¼‰
- warmupï¼šæ¯ä¸ªconfigçš„warmupæ—¶é—´ï¼Œé»˜è®¤25ms
- repï¼šæ¯ä¸ªconfigçš„é‡å¤æ—¶é—´ï¼Œé»˜è®¤100ns

```python
@triton.autotune(
    configs=[
        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),
        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),
        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),
    ],
    key=['M', 'N', 'K'],
)
```

ï¼ˆ1ï¼‰å½“å‰**è¦æ±‚æ‰€æœ‰BLOCK_SIZEè®¾ç½®çš„å€¼éƒ½å¾—æ˜¯2æ¬¡å¹‚**ï¼Œå› ä¸ºåœ¨gpuä¸Šæ•°æ®ä¸º2æ¬¡å†ªçš„è§„æ¨¡æ—¶æ€§èƒ½æ›´å¥½ã€‚

```python
n_rows, n_cols = x.shape
BLOCK_SIZE = triton.next_power_of_2(n_cols)
```

ï¼ˆ2ï¼‰æš´åŠ›ç»†ç²’åº¦tuneæ—¶é—´å¤ªå¸¸ï¼Œå¯ä»¥é€šè¿‡ `prune_configs_by` å‡æ

```python
@triton.autotune(
    configs=cfggen_reduce_op(),
    key=["M"],
    prune_configs_by={'early_config_prune': prune_reduce_config}
)
```

ä»»ä½•è®¾ç½®è¿™ä¸ª prune_func ? æ ¹æ®ç»éªŒä»¥åŠç¡¬ä»¶ã€‚

```python
def prune_reduce_config(configs, named_args, **kwargs):
    M = named_args["M"]
    pruned_configs = []
    for config in configs:
        BLOCK_SIZE = config.kwargs["BLOCK_SIZE"]
        num_stages = config.num_stages
        num_block = M // BLOCK_SIZE
        ...
    if (len(pruned_configs) == 0):
        pruned_configs.append(triton.Config({"BLOCK_SIZE": next_power_of_two(M)}, num_warps=4, num_stages=1))
    return pruned_configs

```

### cache

> `triton cache` é»˜è®¤å­˜åœ¨ `~/.triton/cache/` æ–‡ä»¶å¤¹ä¸‹ï¼Œå½“ç„¶ä¹Ÿå¯ä»¥ä½¿ç”¨ `export TRITON_CACHE_DIR=xxx` æ¥æŒ‡å®šã€‚

triton æ˜¯ jit çš„æ‰§è¡Œæ¨¡å¼ï¼Œä½†ä¸ºäº†å‡å°‘ç¼–è¯‘æ—¶é—´ï¼Œå…¶å®ä¼šä¿ç•™æ¯æ¬¡ç¼–è¯‘å¾—åˆ°çš„ kernelï¼Œæ‰€ä»¥çœŸå®çš„ç¼–è¯‘æµç¨‹æ˜¯ï¼š

1.æ ¹æ® kernel ä¿¡æ¯ç”Ÿæˆä¸€ä¸ª `metadata_filename`(ä¸€ä¸ª json æ–‡ä»¶)ï¼Œç„¶ååœ¨cacheç›®å½•ä¸­æŸ¥æ‰¾

```python
    if not always_compile and metadata_path is not None:
        # cache hit!
        metadata = json.loads(Path(metadata_path).read_text())
        return CompiledKernel(src, metadata_group, hash) # hash
```

2.å¦‚æœæ‰¾åˆ°äº†å°±ç”¨ï¼Œæ²¡æ‰¾åˆ°å°±éœ€è¦ç¼–è¯‘

python->ast->ttir->...

```python
â”œâ”€â”€ 1e888eeebd0dc80dd6656c3193fb8376020f69d0e3b0b3f8a540f22c12357303
â”‚   â”œâ”€â”€ _attn_fwd.cubin
â”‚   â”œâ”€â”€ _attn_fwd.json
â”‚   â”œâ”€â”€ _attn_fwd.llir
â”‚   â”œâ”€â”€ _attn_fwd.ptx
â”‚   â”œâ”€â”€ _attn_fwd.ttgir
â”‚   â”œâ”€â”€ _attn_fwd.ttir
â”‚   â””â”€â”€ __grp___attn_fwd.json
â”œâ”€â”€ 41ce1f58e0a8aa9865e66b90d58b3307bb64c5a006830e49543444faf56202fc
â”‚   â””â”€â”€ cuda_utils.so
â””â”€â”€ 72d7b51280ef4fc8848331fd56d843a8a4deab4657a8767807c06728dbc23691
    â””â”€â”€ __triton_launcher.so
```

3.æŸä¸ªopå¤šæ¬¡æ‰§è¡Œï¼Œä»€ä¹ˆæ—¶å€™ä¼šhit cacheï¼Œä»€ä¹ˆæ—¶å€™éœ€è¦é‡æ–°ç¼–è¯‘å‘¢ï¼Ÿ

è¿™éœ€è¦ä» triton ä½•æ—¶ä¼šäº§ç”Ÿä¸€ä¸ªæ–° cache è®²èµ·ã€‚ triton ä¼šä»¥ [key](https://github.com/triton-lang/triton/blob/main/python/triton/runtime/jit.py#L583) ä¸ºæ ¸å¿ƒï¼Œkey åŒ…å« `sig_and_spec`, `constexpr_vals` å’Œ `excess_kwargs`ã€‚

- `sig_and_spec`ï¼š å‡½æ•°å å’Œ å‚æ•°ç±»å‹ï¼Œç›´æ¥è¡¨ç°åœ¨ kernel ä¸Šã€‚å½“å‚æ•°ç±»å‹å¾ˆå¤šçš„æ—¶å€™ä¹Ÿå¯ä»¥ä½¿ç”¨ `do_not_specialize` æ¥æ’é™¤æ‰æŸäº›å‚æ•°çš„å½±å“ï¼Œæ¥é¿å…ç”Ÿæˆæ›´å¤šçš„ kernelã€‚
- `constexpr_vals`ï¼š æ ‡è®°ä¸º `tl.constexpr` çš„å‚æ•°
- `excess_kwargs`ï¼š`num_stages`, `num_warps`, `num_stages` ç­‰

ç¼“å­˜ `autotune` ä¸­æ€§èƒ½æœ€å¥½çš„ config ç”Ÿæˆçš„ kernelã€‚

## backend

### launch

ç”±äºæ¯ä¸ª kernel çš„å‚æ•°å¯èƒ½ä¸åŒï¼Œæ‰€ä»¥éœ€è¦ä¸ºå…¶ç”Ÿæˆä¸åŒçš„æ‰§è¡Œå‡½æ•°ï¼Œä¼šä½¿ç”¨ kernel çš„ä¸€äº›ä¿¡æ¯å’Œ[å›ºå®šçš„ä»£ç æ‹¼æ¥](https://github.com/triton-lang/triton/blob/main/third_party/nvidia/backend/driver.py#L147)ã€‚æœ€ç»ˆå˜æˆä¸€ä¸ªå¯ä»¥è°ƒä»¥è°ƒç”¨çš„æ¥å£ã€‚

# triton-lang elements

è¿™é‡Œåªæ˜¯ç®€å•ä»‹ç»ä¸‹ï¼Œä¸¾ä¸ªğŸŒ°ï¼Œvector add

```python
import torch
import triton
import triton.language as tl

# è¿™æ˜¯kernel
@triton.jit
def add_kernel(x_ptr,  # *Pointer* to first input vector.
               y_ptr,  # *Pointer* to second input vector.
               output_ptr,  # *Pointer* to output vector.
               n_elements,  # Size of the vector.
               BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process.
               # NOTE: `constexpr` so it can be used as a shape value.
               ):
    # There are multiple 'programs' processing different data. We identify which program
    # we are here:
    pid = tl.program_id(axis=0)  # We use a 1D launch grid so axis is 0.
    # This program will process inputs that are offset from the initial data.
    # For instance, if you had a vector of length 256 and block_size of 64, the programs
    # would each access the elements [0:64, 64:128, 128:192, 192:256].
    # Note that offsets is a list of pointers:
    block_start = pid * BLOCK_SIZE
    offsets = block_start + tl.arange(0, BLOCK_SIZE)
    # Create a mask to guard memory operations against out-of-bounds accesses.
    mask = offsets < n_elements
    # Load x and y from DRAM, masking out any extra elements in case the input is not a
    # multiple of the block size.
    x = tl.load(x_ptr + offsets, mask=mask)
    y = tl.load(y_ptr + offsets, mask=mask)
    output = x + y
    # Write x + y back to DRAM.
    tl.store(output_ptr + offsets, output, mask=mask)

# è¿™æ˜¯launchå‡½æ•°
def add(x: torch.Tensor, y: torch.Tensor):
    # We need to preallocate the output.
    output = torch.empty_like(x)
    assert x.is_cuda and y.is_cuda and output.is_cuda
    n_elements = output.numel()
    # The SPMD launch grid denotes the number of kernel instances that run in parallel.
    # It is analogous to CUDA launch grids. It can be either Tuple[int], or Callable(metaparameters) -> Tuple[int].
    # In this case, we use a 1D grid where the size is the number of blocks:
    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']), )
    # NOTE:
    #  - Each torch.tensor object is implicitly converted into a pointer to its first element.
    #  - `triton.jit`'ed functions can be indexed with a launch grid to obtain a callable GPU kernel.
    #  - Don't forget to pass meta-parameters as keywords arguments.
    add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=1024)
    # We return a handle to z but, since `torch.cuda.synchronize()` hasn't been called, the kernel is still
    # running asynchronously at this point.
    return output
```

## input

- pointer

`x_ptr, y_ptr` æŒ‡é’ˆï¼Œä¸ºå…¶ä»£è¡¨çš„tensorçš„ç¬¬ä¸€ä¸ªå…ƒç´ çš„åœ°å€ã€‚ç”¨æ¥å°†æ•°æ®loadåˆ°memory

- hyper-parameter

è¶…å‚æ•° `tl.constexptr` ï¼Œè¿è¡Œæ—¶ä¼ å…¥çš„å€¼æ¥è‡ª compiler æœç´¢å¾—åˆ°ã€‚å¯¹äºä¸åŒçš„ç¡¬ä»¶ä½¿ç”¨æ—¶ï¼Œæœ€ä½³æ€§èƒ½ï¼ˆè®¿å­˜+è®¡ç®—ï¼‰çš„å‚æ•°å¯èƒ½æ˜¯ä¸åŒçš„ï¼Œåç»­ç”± Triton compiler æ¥è¿›è¡Œæœç´¢ä¸åŒçš„å€¼

- stride

è¾“å…¥ä¸­ä¸€èˆ¬ä¹Ÿæœ‰strideï¼Œå¯¹äºnç»´çš„tensor aï¼Œa.stride()ä¼šè¾“å‡ºä¸€ä¸ªnç»´æ•°ç»„ã€‚strideç”¨æ¥æ‰¾æ¯ä¸ªå…ƒç´ çš„æŒ‡é’ˆ

## pid

ä¸€ä¸ª job è¢«åˆ†æˆ grid ä¸ª taskã€‚`pid = tl.program_id(axis=0)` è¡¨ç¤ºå½“å‰æ˜¯ç¬¬å‡ ä¸ªtask

1. program_idæ˜¯è¿™ä¸ªè™šæ‹Ÿçš„**for å¾ªç¯ é‡Œé¢çš„ index** (ç¬¬å‡ æ¬¡å¾ªç¯ï¼Œå®é™…ä¸­è¿™äº›å¾ªç¯æ˜¯å¹¶è¡Œ)

```python
pid = tl.program_id(axis=0)
# å½“è®¿é—®æ•°æ®æ€»é•¿256, BLOCK_SIZE=64
# tl.arange(0, BLOCK_SIZE) -> [0, 63]
# 0ï¼Œ 64ï¼Œ 128ï¼Œ 192
block_start = pid * BLOCK_SIZE
# æ‰€ä»¥æ•°æ®è®¿é—®æ—¶æ˜¯æŒ‰ç…§ [0:64, 64:128, 128:192, 192:256]
offsets = block_start + tl.arange(0, BLOCK_SIZE)
```

2. `axis`Â , æ˜¯è¯´æ˜ "å¾ªç¯"æœ‰å‡ å±‚ï¼Œæ­¤å¤„ axis = 0è¡¨ç¤ºå±•å¼€ä¸º1ç»´æ¥è®¿é—®ï¼ˆç»´åº¦æ¦‚å¿µç±»æ¯”memrefçš„ç»´åº¦ï¼Œç¬¬ä¸€ç»´ç›¸å½“äºmemrefçš„æœ€å†…ç»´uï¼‰

axisæ˜¯å¯åŠ¨3d Gridçš„ç´¢å¼•ï¼Œå¿…é¡»æ˜¯0 / 1 / 2

## load & store

æ˜¾ç¤ºåœ°loadå’Œstoreï¼Œæ‰¹é‡æ•°æ®å¤„ç†ï¼Œä»¥BLOCKä¸ºå•ä½ï¼Œä¸€æ¬¡å¤„ç†ä¸€ä¸ªBLOCK_SIZEçš„æ•°æ®ï¼ŒSIMDè¡Œä¸º

loadï¼šä»DRAMè¯»åˆ°SRAMï¼›storeï¼šä»SRAMå†™å›DRAMï¼›å‡å°‘äº†DRAMä¸Šè®¡ç®—è¡Œä¸ºã€‚loadå’Œstoreåœ¨è¯­ä¹‰ä¸Šå¯ä»¥è§£é‡Šä¸ºgatherå’Œscatter

```python
# load å’Œ store æ—¶éƒ½æ˜¯ä½¿ç”¨åŸºåœ°å€åŠ åç§» è·å¾—ä¸€ç‰‡æ•°æ®ï¼Œmaskè¡¨ç¤ºåªè·å¾—è¿™ç‰‡æ•°æ®ä¸­çš„ä¸€éƒ¨åˆ†
x = tl.load(x_ptr + offsets, mask=mask)
y = tl.load(y_ptr + offsets, mask=mask)
# å†™å›æ—¶ä¹Ÿéœ€è¦mask
tl.store(output_ptr + offsets, output, mask=mask)
```

- offsets

offsets ä¸ºè¦å¤„ç†æ•°æ®çš„èŒƒå›´ï¼Œç”±å½“å‰block_startå’Œrangeè®¡ç®—è€Œæˆ

```python
block_start = pid * BLOCK_SIZE
offsets = block_start + tl.arange(0, BLOCK_SIZE)
```

- mask

mask ä¸ºé®ç›–ï¼Œç±»ä¼¼decoder Attnä¸­çš„maskã€‚ä¸€æ˜¯è§„èŒƒè®¿å­˜è¡Œä¸ºï¼Œé˜²æ­¢è¶Šç•Œï¼ˆæœ€åä¸€å—æ•°æ®å¤§å°å¯èƒ½ä¸æ»¡è¶³é˜²æ­¢ BLOCK_SIZE çš„å¤§å°ï¼‰ï¼›äºŒæ˜¯è¿‡æ»¤å¯¹æœ¬æ¬¡è®¡ç®—ä¸å¿…é¡»çš„æ•°æ®ã€‚

ä¾‹å¦‚offset=1024ï¼Œmaskä¸ºä¸€ä¸ª1024ç»´çš„æ•°ç»„ï¼Œæ¯ä¸ªæ•°ä¸º0/1ï¼Œå½“æŸä½ä¸º1æ—¶ï¼Œåˆ™loadè¯¥æ•°æ®ï¼Œå½“æŸä½ä¸º0æ—¶ï¼Œèˆå¼ƒã€‚

## grid

è°ƒç”¨kernelæ—¶ï¼Œéœ€è¦è¯´æ˜è¯¥kernelæ‰§è¡Œå¾ªç¯æœ‰å‡ å±‚ï¼Œæ¯å±‚æœ‰å‡ æ¬¡ï¼Œè¿™å°±æ˜¯ `grid` çš„æ¦‚å¿µ

ä»¥Matmulè€Œè¨€ï¼Œè‹¥Aä¸ºMxKï¼ŒBä¸ºKxNï¼Œé‚£ä¹ˆCçš„å¤§å°å°±æ˜¯MxNï¼ˆMå’ŒNä¸ºparallel axiså¤§å°ï¼ŒKä¸ºreductionè½´å¤§å°ï¼‰

æ¯æ¬¡åˆ†å—è®¡ç®—ï¼Œå•å—å¤§å°BLOCK_SIZE_M x BLOCK_SIZE_Nï¼Œæ€»å…±è¿›è¡Œ
$$
\frac{M}{\text{BLOCK\_{SIZE}\_{M}}} \times \frac{N}{\text{BLOCK\_{SIZE}\_{N}}}
$$
Tritonä¸­å…³äºgridå®šä¹‰ï¼š

```python
    grid = lambda META: (
        triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),
    )
    matmul_kernel[grid](
        a, b, c,
        M, N, K,
        a.stride(0), a.stride(1),
        b.stride(0), b.stride(1),
        c.stride(0), c.stride(1),
        ACTIVATION=activation
    )
```

å¯¹æ¯”Cudaä¸­launch kernelçš„è¡Œä¸º

```cpp
  dim3 block(BLOCK_SIZE_M, BLOCK_SIZE_N);
  dim3 grid((M + BLOCK_SIZE_M - 1) / BLOCK_SIZE_M, (N + BLOCK_SIZE_N - 1) / BLOCK_SIZE_N);
  matmul_kernel<<<grid,block>>>(Ad, Bd, Cd, M, N, K);
```

## num_warp

ä¸€èˆ¬ä½“ç°åœ¨module Atträ¸Šï¼Œä¸‹é¢çš„ä»£ç æ„å‘³ç€è¿™ä¸ªç¨‹åºä½¿ç”¨4ä¸ªwarpæ‰§è¡Œï¼ˆè¿™ä¸ªå‚æ•°ä¸€èˆ¬ä¹Ÿæ˜¯ `tl.constexpr`ï¼‰

```python
"triton_gpu.num-warps" = 4 : i32
```

tritongpu irç›¸æ¯”ttirä»…å¤šäº†ä¸€ä¸ªBlocked Layoutï¼Œæœ¬è´¨ä¸Šæè¿°çš„æ˜¯Blockå¯¹Memoryçš„Access Pattern

```python
 #blocked = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>
```

å°±æ˜¯**ä¸€ä¸ªCTAé‡Œæœ‰4ä¸ªWarpï¼Œä¸€ä¸ªWarpæœ‰32ä¸ªThreadï¼Œä¸€ä¸ªThreadå¤„ç†1ä¸ªå…ƒç´ **ã€‚

Blocked Layoutåªæ˜¯ä¸€ç§Patternï¼Œä½†**æŒ‰ç…§è¿™ä¸ªPatternä¼šå¤šæ¬¡è®¿é—®ï¼Œæ€»è®¿é—®é‡è¾¾åˆ°BLOCK_SIZE**

## num_stages

æ§åˆ¶è½¯ä»¶æµæ°´å±•å¼€çº§æ•°

tritonä¸­çš„æµæ°´ä½äº[TritonGPU/Transforms/Pipeliner](https://github.com/triton-lang/triton/tree/main/lib/Dialect/TritonGPU/Transforms/Pipeliner)ã€‚

Triton ä¸­å¹¶ä¸éœ€è¦å¾ˆå¤æ‚çš„è½¯ä»¶æµæ°´

- gridä¸€èˆ¬å†™æˆ(M/BLOCK_SIZE, 1, 1)ï¼Œæ¯ä¸ªjobå†…çš„æ•°æ®é‡è¾ƒå°
- SMä¸­çš„ warp-schedule èµ·åˆ°äº†ä¸€äº›æµæ°´çš„æ•ˆæœï¼šå½“warpæ‰§è¡Œä¸€ä¸ªlong-latencyçš„æŒ‡ä»¤æ—¶ï¼Œä¼šé€šè¿‡åˆ‡æ¢warpæ©ç›–æŒ‡ä»¤å¼€é”€
- NVåœ¨æ–°æ¶æ„ä¸­å¼•å…¥äº† `cp.async` æŒ‡ä»¤ï¼Œgdram -> shared memory çš„æ‹·è´å¯ä»¥é€šè¿‡DMAå¼‚æ­¥è¿›è¡Œ -> æ‰€ä»¥éœ€è¦è½¯ä»¶æµæ°´æ¥ä¼˜åŒ–loadçš„è¡Œä¸ºï¼Œè®©å¾ªç¯å†…çš„loadå°½é‡å¯ä»¥å¼‚æ­¥

-> åªéœ€è¦è€ƒè™‘loadæƒ…å†µï¼Œè®¡ç®—å’Œè®¿å­˜æµæ°´é warp schedulerè¿™ä¸€å±‚æ¥å¤„ç†

ä¸€ä¸ªmatmulçš„è®¡ç®—è¡¨ç¤ºir

```text
scf.for %arg0 = %c0 to %size step %c1 {
    load.sync A & B
    dot A, B
}
```

å½“num_stagses = 2çš„æ—¶å€™ï¼Œç›¸å½“äºä¼šæå‰åŠ è½½ä¸€ä¸ªiterçš„æ•°æ®ï¼Œä¹Ÿå°±æ˜¯å˜æˆå¦‚ä¸‹ä»£ç :

```text
load.async A0, B0
async_wait
scf.for %arg0 = %c1 to %size-1 step %c1 { %arg0=A0, %arg1=B0 }{
    dot arg0, arg1
    addptr Anext, Bnext
    load.async Anext, Bnext
    async_wait
}
async_wait
```

æµæ°´ä¸­ä½¿ç”¨çš„ä¸€äº›ir

- triton_gpu.alloc_tensor : tensor<128x32xf32, #share> ç”³è¯·shared memory
- triton_gpu.insert_slice_async %src, %dst, %index ï¼š tensor<128x32x!tt.ptr<f32>> â†’ tensor<128x32xf32, #share> åº•å±‚å¯¹åº”çš„cp.asyncæŒ‡ä»¤
  - %src: æ•°æ®åœ°å€ï¼Œä¹Ÿå°±æ˜¯tensor<128x32x!tt.ptr<f32>>
  - %dst: ç”³è¯·çš„shared memory
  - %index: ä»srcåŠ è½½çš„æ•°æ®æ’å…¥åˆ°%dstçš„ç›®æ ‡ç´¢å¼•
- triton_gpu.async_commit_group åº•å±‚å¯¹åº”çš„ cp.async.commit_group æŒ‡ä»¤ï¼Œå°±æ˜¯å°†å‰é¢çš„æ‰€æœ‰ cp.async æŒ‡ä»¤æ‰“åŒ…åˆ°ä¸€èµ·æ‰§è¡Œ
- triton_gpu.async_wait {num} åº•å±‚å¯¹åº”çš„ cp.async.wait_group æŒ‡ä»¤ï¼Œä¹Ÿå°±æ˜¯ç­‰å¾…å‰é¢ num ä¸ª cp.async-groups æ‰§è¡Œå®Œ

# trick

## ä¸åŒçš„ç¯å¢ƒå˜é‡

- `MLIR_ENABLE_DUMP=1`

dumps the IR before every MLIR pass Triton runs

- `TRITON_PRINT_AUTOTUNING=1`

æ‰“å°æ¯æ¬¡é€‰æ‹©çš„ config

- `TRITON_INTERPRET=1`

é€‚ç”¨ numpy **è§£é‡Šæ‰§è¡Œ**ï¼Œç›´æ¥å˜æˆä¸€ä¸ªcpu kernelï¼Œç”¨æ¥éªŒè¯ç®—æ³•çš„å‡†ç¡®æ€§ã€‚

å¯ä»¥æ–¹ä¾¿åœ°ä½¿ç”¨ pdb å’Œ printï¼Œé’ˆå¯¹ load å’Œ store éƒ½é‡æ–°å®ç°äº†ï¼Œä½†ä¸æ”¯æŒ atomicï¼Œä¸æ”¯æŒå¤šçº§æŒ‡é’ˆ(`!tt.ptr<!tt.ptr<f32>>`)ã€‚

- `TRITON_PRINT_AUTOTUNING=1`

æ‰“å°æ¯æ¬¡ tuning ä¸­çš„æœ€ä¼˜ tuning config å’Œ è€—æ—¶ã€‚

## æ‰“å°passå‰åir

- ä½¿ç”¨ `triton-opt` ç›´æ¥è·‘ pipelineï¼ŒåŠ ä¸Š `mlir-print-ir-after-all`
- æ”¹ python ä¸­ triton åº“ `site-packages/triton/backend/xxx/compiler.py` ä¸­çš„ä»£ç ï¼Œä¾‹å¦‚æ³¨é‡Šæ‰ä¸‹é¢æŸä¸ªpassæ¥çœ‹iræ˜¯å¦ä¸åŒ

```python
    # /usr/lib/python3.10/site-packages/triton/backends/xxx/compiler.py
    def make_ttir(mod, metadata, opt):
        pm = ir.pass_manager(mod.context)
        pm.enable_debug()
        passes.common.add_inliner(pm)
        passes.ttir.add_rewrite_tensor_pointer(pm)
        passes.ttir.add_combine(pm)
        passes.common.add_canonicalizer(pm)
        passes.ttir.add_reorder_broadcast(pm)
        passes.common.add_cse(pm)
        passes.common.add_licm(pm)
        passes.common.add_symbol_dce(pm)
        pm.run(mod)
        return mod
```

# ä¼˜åŒ–

## å¸¸è§æ–¹æ³•

1.è°ƒæ•´tuning config

å…ˆæŠŠç²’åº¦è°ƒç»†ï¼Œç¡®å®šå½“å‰ä»»åŠ¡è§„æ¨¡çš„æœ€ä¼˜æ€§èƒ½å¤§è‡´é€‰æ‹©çš„tuning configï¼Œå†å†™å‡æå‡½æ•°ã€‚

- è°ƒæ•´æ‹†åˆ†å¤§å°ï¼šBLOCK_SIZE
- num_stagesè°ƒæ•´æµæ°´çº§æ•°

2.ä¿®æ”¹kernelå®ç°

- æ¯ä¸ªç®—å­çš„ç†è®ºè®¡ç®—é‡æ˜¯å›ºå®šçš„ï¼Œä¸€èˆ¬éƒ½æ˜¯æœ‰å†—ä½™çš„IOã€‚
- å¤šç”¨make_tensor_ptrï¼Œäº§ç”Ÿ`!tt.ptr<tensor<xxxx>>`ï¼Œç›´æ¥æ˜¯ä¸€ç‰‡è¿ç»­çš„æ•°å­—

3.æ·»åŠ æ–°çš„ä¸‹é™pattern

ä¸€èˆ¬æ¶‰åŠä¸åˆ°ã€‚elementwise(broadcast(a), broadcast(b)) -> broadcast(elementwise(a, b))

## ç¤ºä¾‹

1.ä¿®æ”¹ç®—æ³•ï¼Œå‡å°‘loadæ•°é‡

layernormçš„æ­£å‘è®¡ç®—ï¼šloadä¸€æ¬¡è®¡ç®—meanï¼Œå†loadä¸€æ¬¡è®¡ç®—var -> `var = sum(x^2) / n - x_hat^2` ä¸€æ¬¡è®¡ç®—meanå’Œvar

```python
@triton.jit
def _layer_norm_fwd_fused(
    ...
    mean = 0
    _mean = tl.zeros([BLOCK_SIZE], dtype=tl.float32)
    for off in range(0, N, BLOCK_SIZE):
        cols = off + tl.arange(0, BLOCK_SIZE)
        a = tl.load(X + cols, mask=cols < N, other=0.).to(tl.float32)
        _mean += a
    mean = tl.sum(_mean, axis=0) / N
    # Compute variance
    _var = tl.zeros([BLOCK_SIZE], dtype=tl.float32)
    for off in range(0, N, BLOCK_SIZE):
        cols = off + tl.arange(0, BLOCK_SIZE)
        x = tl.load(X + cols, mask=cols < N, other=0.).to(tl.float32)
        x = tl.where(cols < N, x - mean, 0.)
        _var += x * x
    var = tl.sum(_var, axis=0) / N
    rstd = 1 / tl.sqrt(var + eps)
    ...
```

`var = sum((x - mean)^2)` -> `var = sum(x^2) / n - mean^2`

```python
# Optimized Implementation.
@triton.jit
def _layer_norm_fwd_fused_with_small_n(
    ...
    # Compute mean and variance
    mean = 0
    _mean = tl.zeros([BLOCK_SIZE], dtype=tl.float32)
    _x_square = tl.zeros([BLOCK_SIZE], dtype=tl.float32)
    for off in range(0, N, BLOCK_SIZE):
        cols = off + tl.arange(0, BLOCK_SIZE)
        a = tl.load(X + cols, mask=cols < N, other=0.).to(tl.float32)
        _mean += a
        _x_square += a * a
    mean = tl.sum(_mean, axis=1) / N
    var = tl.sum(_x_square, axis=1) / N - mean * mean
    rstd = 1 / tl.sqrt(var + eps)
    ...
```

2.åˆå¹¶kernelï¼Œè°ƒèŠ‚config

ä¾‹ï¼šæ¥æºï¼š[flaggems/vector_norm](https://github.com/FlagOpen/FlagGems/blob/master/src/flag_gems/ops/vector_norm.py#L146C1-L168C23)

```python
@triton.jit
def min_norm_kernel_1(X, Mid, M, BLOCK_SIZE: tl.constexpr):
    pid = tl.program_id(0)
    offset = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)
    X = X + offset
    Mid = Mid + pid
    mask = offset < M

    x = tl.load(X, mask=mask, other=float("inf")).to(tl.float32)
    mid = tl.min(tl.abs(x))
    tl.store(Mid, mid)

@triton.jit
def min_norm_kernel_2(Mid, Out, MID_SIZE, BLOCK_MID: tl.constexpr):
    offset = tl.arange(0, BLOCK_MID)
    Mid = Mid + offset
    mask = offset < MID_SIZE
    mid = tl.load(Mid, mask=mask, other=float("inf")).to(tl.float32)
    out = tl.min(mid)
    tl.store(Out, out)

# ä¼ å…¥kernelçš„å‚æ•°
BLOCK_SIZE = triton.next_power_of_2(math.ceil(math.sqrt(M)))
MID_SIZE = triton.cdiv(M, BLOCK_SIZE) # grid
BLOCK_MID = triton.next_power_of_2(MID_SIZE)
mid = torch.empty([MID_SIZE], dtype=dtype, device=x.device)
out = torch.empty(shape, dtype=dtype, device=x.device)
min_norm_kernel_1[(MID_SIZE,)](x, mid, M, BLOCK_SIZE)
min_norm_kernel_2[(1,)](mid, out, MID_SIZE, BLOCK_MID)
```

->

```python
def cfggen_reduce_op():
    block_size = [1024, 2048, 4096, ...]
    num_stage = [...]
    configs=[
        triton.Config({"BLOCK_SIZE": m}, num_warps=4, num_stages=s) for m in block_size for s in num_stage
    ]
    return configs

def prune_reduce_config(configs, named_args, **kwargs):
    M = named_args["M"]
    pruned_configs = []
    for config in configs:
        BLOCK_SIZE = config.kwargs["BLOCK_SIZE"]
        num_stages = config.num_stages
        num_block = M // BLOCK_SIZE
        ...
    if (len(pruned_configs) == 0):
        pruned_configs.append(triton.Config({"BLOCK_SIZE": next_power_of_two(M)}, num_warps=4, num_stages=1))
    return pruned_configs

@triton.autotune(
    configs=cfggen_reduce_op(),
    key=["M"],
    prune_configs_by={'early_config_prune': prune_reduce_config}
)
@triton.jit
def min_norm_kernel(
    X,
    Out,
    M,
    BLOCK_SIZE: tl.constexpr,
):
    pid = tl.program_id(0)
    block_start = pid * BLOCK_SIZE
    offsets = block_start + tl.arange(0, BLOCK_SIZE)
    mask = offsets < M
    x = tl.load(X + offsets, mask, other=0.0).to(tl.float32)
    mid = tl.min(tl.abs(x))
    tl.atomic_min(Out, mid.to(tl.float32))

# ä¼ å…¥kernelçš„å‚æ•°
grid = lambda meta: (triton.cdiv(M, meta['BLOCK_SIZE']), )
out = torch.zeros(shape, dtype=torch.float, device=x.device)
min_norm_kernel_1[grid](x, out, M)
```

3.commonï¼šLICMï¼Œå‡å°‘è·³è½¬å¼€é”€

æ”¹è¿›å†™æ³•è¦å‡å°‘ H*W - 1æ¬¡è·³è½¬å¼€é”€

```python
for i in range(H):
    for j in range(W):
        if (a > b):
            compute1()
        else:
            compute2()

->

if (a > b):
    for i in range(H):
        for j in range(W):
            compute1()
else:
    for i in range(H):
        for j in range(W):
            compute2()
```

# IR

triton python è¯­è¨€æ˜¯pythonçš„ä¸€ä¸ªå­é›†ï¼Œå®ƒé€šè¿‡ast æ¨¡å—è§£æpythonä»£ç ï¼Œç”Ÿæˆä¸€ä¸ªpython è¯­æ³•æ ‘ã€‚

![AST](/assets/img/blog/img_triton_survey/ast.png)

éå†æ•´ä¸ªè¯­æ³•æ ‘çš„è¿‡ç¨‹ï¼Œé€šè¿‡code_generatorç›¸å…³ä»£ç ï¼Œå®šä¹‰visit_Assignç­‰å‡½æ•°ï¼Œé€šè¿‡mlir builderç”Ÿæˆå¯¹åº”çš„mlir(ttir)ã€‚ç„¶åttirç»§ç»­èµ° mlir pipelineç»§ç»­ä¸‹é™ã€‚

![dialect](/assets/img/blog/img_mlir_gpu_pipeline_component/nvvm_dialect_ir.png)

## æ·»åŠ ä¸€ç§æ–°åŸè¯­

å‚è€ƒï¼š[Implement scaled_dot(mxfp8, fp8) via mma](https://github.com/triton-lang/triton/pull/4795)

dot_scaled -> semantic.dot_scaled -> tl.tensor(builder.create_dot_scaled(...)ï¼‰

1.`python/triton/language/core.py` ä¸­æ³¨å†Œè¯¥åŸè¯­ï¼Œreturnéƒ¨åˆ†ä¼šè°ƒç”¨åˆ° semantic ä¸­

2.`python/triton/language/semantic.py` æè¿°äº†è¯¥åŸè¯­åˆ›å»º ir çš„è¡Œä¸ºï¼Œä¾‹å¦‚å¯¹operandè¿›è¡Œ broadcast æˆ– splat

3.`python/src/ir.cc` åˆ›å»ºtriton dialect op(tt.op)

4.æ·»åŠ åç»­conversion(ttir->ttgir->llvmir)

## AxisInfo

`AxisInfo` ä¼šå¯¹ loadã€store ç­‰å¯¹æŒ‡é’ˆè¿›è¡Œæ“ä½œçš„ op åŠç›¸å…³ op è¿›è¡Œè·Ÿè¸ªï¼Œåˆ†æå‡º `divisibility, contiguity, constancy` ä¿¡æ¯ï¼Œä»è€Œè¾…åŠ©ä¹‹åçš„ pass è¿›è¡Œï¼Œä»¥è·å¾—é«˜ IO æ•ˆç‡ã€‚

- divisibilityï¼šç»´åº¦iä¸Šï¼Œæ‰€æœ‰å…ƒç´ çš„æœ€å¤§äºŒæ¬¡å¹‚å…¬çº¦æ•°ã€‚è¯¥å€¼ç”¨æ¥è¡¨ç¤ºæŒ‡é’ˆåœ°å€çš„å¯¹é½ï¼ŒæŒ‡å¯¼åç»­è®¿å­˜æ“ä½œçš„ä¸‹é™
- contiguityï¼šç»´åº¦iä¸Šï¼Œè¿ç»­å…ƒç´ çš„æœ€å°é•¿åº¦ï¼Œè¯´æ˜è‡³å°‘æ¯contiguity[i]ä¸ªå…ƒç´ æ˜¯è¿ç»­çš„
- constancyï¼šç»´åº¦iä¸Šï¼Œé‡å¤å…ƒç´ çš„æœ€å°é•¿åº¦ï¼Œè¯´æ˜è‡³å°‘æ¯constancy[i]ä¸ªå…ƒç´ æ˜¯é‡å¤çš„
- constantValueï¼šè¡¨ç¤ºå€¼ä¸ºå¸¸æ•°ï¼Œä¸€èˆ¬ä¿¡æ¯æºå¤´æ¥è‡ª `arith.constant`

åˆ©ç”¨è¿™äº›ä¿¡æ¯å¯ä»¥ä¼˜åŒ–æ”¾å­˜è¡Œä¸ºï¼š

- å½“ load 16x256 ä¸ªæ•°æ—¶ï¼Œè‹¥ä½ç»´æ¯ 256 ä¸ªæ•°é‡å¤ï¼Œé‚£ä¹ˆå¯ä»¥åª load 16 ä¸ªæ•°æ®ï¼Œå† broadcast æˆ 16x256ã€‚
- maskå…¨ä¸ºtrueæˆ–falseæ—¶å¯ä»¥è½¬ä¸º scf.if + loadï¼Œåä¹‹ä¹Ÿå¯ä»¥ä¼˜åŒ–ä¸º `select %mask, %load, %other`

å¯ä»¥é€šè¿‡ `test-print-alignment` pass æ¥æ‰“å° `AnisInfo`ï¼Œè¯¦è§ [TestAxisInfo](https://github.com/triton-lang/triton/blob/main/test/lib/Analysis/TestAxisInfo.cpp)

## layout

TritonGPU Dialect è¿™ä¸€å±‚çš„ IRçš„ tensor è¡¨ç¤ºå°†å¸¦æœ‰ Layout çš„ Attrï¼Œè¯¥ Attr å®šä¹‰äº† Data æ˜¯å¦‚ä½•è¢« Thread å¹¶è¡Œå¤„ç†ã€‚è¿™ç§layout attråœ¨loweringè¿‡ç¨‹ä¸­è¢«ä¼ é€’ã€‚

è‡ª [[RFC] To generalize TritonGPU dialect for GPU of different vendors](https://github.com/triton-lang/triton/issues/2639) åï¼Œ TritonGPU Dialect ä¸­çš„ Layout Attribute æ­£å¼ç»Ÿä¸€ä¸ºä¸‹å›¾ï¼š

![TritonGPU Attr](/assets/img/blog/img_triton_pass/layout_attr.png)

å½“å‰æœ€é‡è¦çš„ä¹Ÿæ˜¯ distributed layout å’Œ shared layoutã€‚

### shared layout

è¡¨ç¤ºäº†ä¸€ç§ tensor ç¼–ç æ¨¡å¼ï¼Œç”¨äºæŒ‡å¯¼å¦‚ä½•è¿›è¡Œ `swizzle`ï¼Œä½¿å¾—ä¸åŒ thread åœ¨è®¿é—® tensor(on shared memory) ä¸Šçš„å…ƒç´ æ—¶å°½é‡é¿å… bank conflictã€‚

> swizzleï¼šè°ƒæ•´æ•°æ®åœ¨ shared memory ä¸Šçš„å­˜å‚¨ä½ç½®ï¼Œä¿è¯ thread è®¿é—®æ—¶ä¸å‡ºç° bank conflictã€‚

è¯¥layoutä¸­ä¸»è¦å¯¹è±¡ä¸ºï¼š

- vecï¼šåŒè¡Œçš„å…ƒç´ è¿›è¡Œswizzleæ—¶ï¼Œè¿ç»­vecä¸ªä¸ºä¸€ç»„
- perPhaseï¼šä¸€æ¬¡ swizzling phase å¤„ç†çš„ row æ•°ã€‚è¿ç»­çš„ perPhase è¡Œç”¨ç›¸åŒçš„swizzleæ–¹æ³•
- maxPhaseï¼šswizzling phaseçš„ä¸ªæ•°
- orderï¼šè¡¨æ˜å“ªä¸€ä¸ªä¸ºä¸»åºï¼Œ[1, 0] ä¸ºè¡Œä¸»åºï¼ŒåŒè¡Œç›¸é‚»å…ƒç´ åœ°å€è¿ç»­
- hasLeadingOffsetï¼šé»˜è®¤ä¸ºfalseï¼ŒHopper MMAv3ä¸ºtrue

swizzle æœ€åŸºç¡€çš„æ–¹æ³•å°±æ˜¯åœ°å€ ä¸ phase_id è¿›è¡Œ xorã€‚

å‡è®¾ç°åœ¨æœ‰4x4ä¸ªå…ƒç´ éœ€è¦å­˜åˆ° shared memory ä¸Šå»ï¼Œå­˜æ”¾çš„åˆå§‹åœ°å€ä¸ºï¼š

[r, c] ä¸Šå¯¹åº”çš„å…ƒç´ ä¸º (r:c)ã€‚

```text
[[(0:0),(0:1),(0:2),(0:3)]
[ (1:0),(1:1),(1:2),(1:3)]
[ (2:0),(2:1),(2:2),(2:3)]
[ (3:0),(3:1),(3:2),(3:3)]]
```

- #shared<{vec=1, perPhase=1, maxPhase=4, order=[1,0]}>

ä¸åŒè¡Œä¸­çš„åœ°å€ä¸ä¸åŒçš„å‚æ•° xorã€‚`out[r][c] = in[r][c ^ r]`

```text
[[(0:0),(0:1),(0:2),(0:3)]  // phase 0 (xor with 0)
[ (1:1),(1:0),(1:3),(1:2)]  // phase 1 (xor with 1)
[ (2:2),(2:3),(2:0),(2:1)]  // phase 2 (xor with 2)
[ (3:3),(3:2),(3:1),(3:0)]] // phase 3 (xor with 3)
```

- #shared<{vec=1, perPhase=2, maxPhase=4, order=[1,0]}>

ç›¸é‚» 2 (`perPhase`) è¡Œä¸­çš„åœ°å€ç”¨ç›¸åŒçš„å‚æ•°xorã€‚`out[r][c] = in[r][c ^ (r / 2)]`

```text
[[(0:0),(0:1),(0:2),(0:3)]   // phase 0 (xor with 0)
[ (1:0),(1:1),(1:2),(1:3)]   // phase 0 (xor with 0)
[ (2:1),(2:0),(2:3),(2:2)]   // phase 1 (xor with 1)
[ (3:1),(3:0),(3:3),(3:2)]]  // phase 1 (xor with 1)
```

- #shared<{vec=2, perPhase=2, maxPhase=4, order=[1,0]}>

ç›¸é‚» 2(vec) å…ƒç´ ä¸ºä¸€ç»„ï¼Œè¿›è¡Œ xorã€‚`out[r][c] = in[r][(c / 2) ^ (r % 2)) * 2 + (c % 2)]`

```text
[[(0:0),(0:1),(0:2),(0:3)]
[ (1:0),(1:1),(1:2),(1:3)]
[ (2:2),(2:3),(2:0),(2:1)]
[ (3:2),(3:3),(3:0),(3:1)]]
```

![swizzled memory](/assets/img/blog/img_triton_survey/swizzled.png)

### distributed layout

distributed layout ä½¿ç”¨æ˜ å°„å‡½æ•°æè¿°æ•´ä¸ª tensor çš„è®¿é—®æ¨¡å¼ã€‚æ˜ å°„å‡½æ•°(layout function)ä¼šå°†ç‰¹å®šçš„Tensoräº¤ç»™ç‰¹å®šçš„Threadå»å¤„ç†(å³ä¸€ä¸ªlayoutæè¿°æ•´ä¸ªtensorçš„è®¿é—®æ¨¡å¼)ï¼Œè¾¾åˆ°ä¸€ä¸ª**distribution**çš„æ•ˆæœ

distributte layout å°†ä¿¡æ¯åˆ†ä¸º4ä¸ªç»´åº¦ï¼š

- CTAs Per CGAï¼šåœ¨ hopper ä¸Šæ‰æœ‰ç”¨ï¼Œå› ä¸º hopper æ¶æ„é¦–æ¬¡å¼•å…¥äº† SM-to-SM
- Warps Per CTAï¼šCTA å†… warp çš„å¸ƒå±€ï¼ˆå¯¹åº” `warpsPerCTA`ï¼‰
- Threads Per Warpï¼šwarp å†… thread çš„å¸ƒå±€ï¼ˆå¯¹åº” `threadsPerWarp`ï¼‰
- Values Per Threadï¼šä¸€ä¸ª thread éœ€è¦å¤„ç†å¤šå°‘å…ƒç´ ï¼ˆå¯¹åº” `sizePerThread`ï¼‰

#### block layout

æœ€å¸¸è§çš„ layoutï¼Œç»“åˆ `AxisInfoAnalysis` è·å¾— load å’Œ store çš„è®¿å­˜è¡Œä¸ºï¼Œå†ç”¨æ¥è®¿å­˜åˆå¹¶(memory coalescing)ï¼Œä½¿å¾—è®¿å­˜è¡Œä¸ºæ›´åŠ é«˜æ•ˆã€‚

An encoding where each warp owns a contiguous portion of the target tensor. This is typically the kind of data layout **used to promote memory coalescing in LoadInst and StoreInst.**

- åŸºç¡€æ¦‚å¿µ

ä¾‹å¦‚ï¼š`#triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [4, 8], warpsPerCTA = [1, 1], order = [1, 0]}>`

- sizePerThread = [1, 4]ï¼šæ¯ä¸ªçº¿ç¨‹å¤„ç†çš„ **è¿ç»­æ’å¸ƒ** æ•°æ®æ•°ç›®
- threadsPerWarp = [4, 8]ï¼š warpå†…çº¿ç¨‹çš„å¸ƒå±€
- warpsPerCTA = [1, 1]ï¼šthread blockå†…warpçš„å¸ƒå±€
- order = [1, 0]ï¼šå…ˆè®¿é—®dim1ï¼Œå†è®¿é—®dim0

è¯¥BLockè®¿å­˜æ¨¡å¼ï¼šæ¯è¡Œç”±8ä¸ªthreadè´Ÿè´£è®¿é—®ï¼Œæ¯ä¸ªthreadä¼šè®¿é—®è¿ç»­4ä¸ªå…ƒç´ ï¼Œæ‰€ä»¥ä¸€æ¬¡èƒ½å¤„ç†(1x4x1, 8x4x1) = (4, 32)è§„æ¨¡çš„shapeã€‚å¦‚ä¸‹ï¼š

```bash
$ triton-tensor-layout -l "#triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [4, 8], warpsPerCTA = [1, 1], order = [1, 0]}>" -t "tensor<4x32xf16>"
# T0:0,  T0:1,  T0:2,  T0:3 è¡¨ç¤º T0 ä¸€æ¬¡å¤„ç†4ä¸ªè¿ç»­æ•°ç»„æˆçš„å—
Print layout attribute: #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [4, 8], warpsPerCTA = [1, 1], order = [1, 0]}>
[[ T0:0,  T0:1,  T0:2,  T0:3,  T1:0,  T1:1,  T1:2,  T1:3,  T2:0,  T2:1,  T2:2,  T2:3,  T3:0,  T3:1,  T3:2,  T3:3,  T4:0,  T4:1,  T4:2,  T4:3,  T5:0,  T5:1,  T5:2,  T5:3,  T6:0,  T6:1,  T6:2,  T6:3,  T7:0,  T7:1,  T7:2,  T7:3]
[  T8:0,  T8:1,  T8:2,  T8:3,  T9:0,  T9:1,  T9:2,  T9:3, T10:0, T10:1, T10:2, T10:3, T11:0, T11:1, T11:2, T11:3, T12:0, T12:1, T12:2, T12:3, T13:0, T13:1, T13:2, T13:3, T14:0, T14:1, T14:2, T14:3, T15:0, T15:1, T15:2, T15:3]
[ T16:0, T16:1, T16:2, T16:3, T17:0, T17:1, T17:2, T17:3, T18:0, T18:1, T18:2, T18:3, T19:0, T19:1, T19:2, T19:3, T20:0, T20:1, T20:2, T20:3, T21:0, T21:1, T21:2, T21:3, T22:0, T22:1, T22:2, T22:3, T23:0, T23:1, T23:2, T23:3]
[ T24:0, T24:1, T24:2, T24:3, T25:0, T25:1, T25:2, T25:3, T26:0, T26:1, T26:2, T26:3, T27:0, T27:1, T27:2, T27:3, T28:0, T28:1, T28:2, T28:3, T29:0, T29:1, T29:2, T29:3, T30:0, T30:1, T30:2, T30:3, T31:0, T31:1, T31:2, T31:3]]
```

ä½†è‹¥è¾“å…¥opçš„shapeä¸º(8, 32)ï¼Œé‚£ä¹ˆè®©æ¯ä¸ªthreadå¤„ç†ä¸¤ä¸ªè¿ç»­å—å³å¯ï¼Œå³ç¬¬ä¸€ä¸ªthreadå¤„ç†(0, 0:3), (4, 0:3)ä¸¤ä¸ªå—ã€‚

#### MMA Layout å’Œ DotOperand Layout

ç”¨æ¥æŒ‡å¯¼ op ä¸‹é™åˆ°ç‰¹æ®ŠæŒ‡ä»¤çš„ attrã€‚

1.MMA Layout

è¡¨ç¤º Tensor Core ä¸­ MMA æŒ‡ä»¤ç»“æœçš„ data layoutï¼Œä¸€èˆ¬å¯ä»¥ç›´æ¥å¯¹åº”åˆ° PTX æŒ‡ä»¤ä¸­ç›¸åº”çš„æ•°æ®æ’å¸ƒéœ€æ±‚ã€‚

> å’Œç¡¬ä»¶ç›¸å…³å¾ˆå¤§ï¼Œè¿™é‡Œä¸å±•å¼€

2.DotOperand Layout

è¡¨ç¤º dotOp çš„è¾“å…¥çš„ layoutã€‚ä¸»è¦åŒ…å« `opIdx` å’Œ `parent` ä¸¤ä¸ªä¿¡æ¯ï¼Œ

- opIdx ï¼šç”¨æ¥æ ‡è¯† dotOp çš„æ“ä½œæ•°
  - opIdx=0 è¡¨ç¤º DotOp çš„ $a
  - opIdx=1 è¡¨ç¤º DotOp çš„ $b
- parentï¼šå†³å®šäº† DotOperand çš„å¸ƒå±€æ–¹å¼
  - MMA Layoutï¼ˆå¦‚æœ DotOp lower åˆ° MMA æŒ‡ä»¤ï¼‰
  - Blocked Layoutï¼ˆå¦‚æœ DotOp lower åˆ° FMA æŒ‡ä»¤ï¼‰

### tools for layout

åœ¨ [PR](https://github.com/triton-lang/triton/pull/4486) ä¸­åˆå…¥äº†ä¸€ä¸ªå¯ä»¥æ‰“å° ttgir ä¸Š layout çš„å·¥å…· `triton-tensor-layout`ï¼Œé€šè¿‡è°ƒç”¨ `getLayoutStr` æ¥è§£æ RankedTensorType ä¸­çš„ layout ä¿¡æ¯ï¼Œä¸”å½“å‰å·²ç»æ”¯æŒäº† shared layout çš„ dumpã€‚ä¾‹å¦‚ï¼š

```bash
$ triton-tensor-layout -l "#triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>" -t "tensor<16x16xf16>"
Print layout attribute: #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>
# æ¯è¡Œç”¨8ä¸ªthreadï¼Œæ¯è¡Œä¸­æ¯ä¸ªthreadä¼šè´Ÿè´£è¿ç»­çš„4ä¸ªå…ƒç´ ï¼Œä½†ä¸€è¡Œåªæœ‰16ä¸ªå…ƒç´ ï¼Œæ‰€ä»¥4ä¸ªthreadå°±èƒ½éå†å®Œä¸€éäº†
# å› æ­¤ T0 å’Œ T4 éƒ½ä¼šè®¿é—®ç¬¬0è¡Œçš„å‰å››ä¸ªå…ƒç´ 
[[  T0:0|  T4:0,   T0:1|  T4:1,   T0:2|  T4:2,   T0:3|  T4:3,   T1:0|  T5:0,   T1:1|  T5:1,   T1:2|  T5:2,   T1:3|  T5:3,   T2:0|  T6:0,   T2:1|  T6:1,   T2:2|  T6:2,   T2:3|  T6:3,   T3:0|  T7:0,   T3:1|  T7:1,   T3:2|  T7:2,   T3:3|  T7:3]
[   T8:0| T12:0,   T8:1| T12:1,   T8:2| T12:2,   T8:3| T12:3,   T9:0| T13:0,   T9:1| T13:1,   T9:2| T13:2,   T9:3| T13:3,  T10:0| T14:0,  T10:1| T14:1,  T10:2| T14:2,  T10:3| T14:3,  T11:0| T15:0,  T11:1| T15:1,  T11:2| T15:2,  T11:3| T15:3]
[  T16:0| T20:0,  T16:1| T20:1,  T16:2| T20:2,  T16:3| T20:3,  T17:0| T21:0,  T17:1| T21:1,  T17:2| T21:2,  T17:3| T21:3,  T18:0| T22:0,  T18:1| T22:1,  T18:2| T22:2,  T18:3| T22:3,  T19:0| T23:0,  T19:1| T23:1,  T19:2| T23:2,  T19:3| T23:3]
[  T24:0| T28:0,  T24:1| T28:1,  T24:2| T28:2,  T24:3| T28:3,  T25:0| T29:0,  T25:1| T29:1,  T25:2| T29:2,  T25:3| T29:3,  T26:0| T30:0,  T26:1| T30:1,  T26:2| T30:2,  T26:3| T30:3,  T27:0| T31:0,  T27:1| T31:1,  T27:2| T31:2,  T27:3| T31:3]
[  T32:0| T36:0,  T32:1| T36:1,  T32:2| T36:2,  T32:3| T36:3,  T33:0| T37:0,  T33:1| T37:1,  T33:2| T37:2,  T33:3| T37:3,  T34:0| T38:0,  T34:1| T38:1,  T34:2| T38:2,  T34:3| T38:3,  T35:0| T39:0,  T35:1| T39:1,  T35:2| T39:2,  T35:3| T39:3]
[  T40:0| T44:0,  T40:1| T44:1,  T40:2| T44:2,  T40:3| T44:3,  T41:0| T45:0,  T41:1| T45:1,  T41:2| T45:2,  T41:3| T45:3,  T42:0| T46:0,  T42:1| T46:1,  T42:2| T46:2,  T42:3| T46:3,  T43:0| T47:0,  T43:1| T47:1,  T43:2| T47:2,  T43:3| T47:3]
[  T48:0| T52:0,  T48:1| T52:1,  T48:2| T52:2,  T48:3| T52:3,  T49:0| T53:0,  T49:1| T53:1,  T49:2| T53:2,  T49:3| T53:3,  T50:0| T54:0,  T50:1| T54:1,  T50:2| T54:2,  T50:3| T54:3,  T51:0| T55:0,  T51:1| T55:1,  T51:2| T55:2,  T51:3| T55:3]
[  T56:0| T60:0,  T56:1| T60:1,  T56:2| T60:2,  T56:3| T60:3,  T57:0| T61:0,  T57:1| T61:1,  T57:2| T61:2,  T57:3| T61:3,  T58:0| T62:0,  T58:1| T62:1,  T58:2| T62:2,  T58:3| T62:3,  T59:0| T63:0,  T59:1| T63:1,  T59:2| T63:2,  T59:3| T63:3]
[  T64:0| T68:0,  T64:1| T68:1,  T64:2| T68:2,  T64:3| T68:3,  T65:0| T69:0,  T65:1| T69:1,  T65:2| T69:2,  T65:3| T69:3,  T66:0| T70:0,  T66:1| T70:1,  T66:2| T70:2,  T66:3| T70:3,  T67:0| T71:0,  T67:1| T71:1,  T67:2| T71:2,  T67:3| T71:3]
[  T72:0| T76:0,  T72:1| T76:1,  T72:2| T76:2,  T72:3| T76:3,  T73:0| T77:0,  T73:1| T77:1,  T73:2| T77:2,  T73:3| T77:3,  T74:0| T78:0,  T74:1| T78:1,  T74:2| T78:2,  T74:3| T78:3,  T75:0| T79:0,  T75:1| T79:1,  T75:2| T79:2,  T75:3| T79:3]
[  T80:0| T84:0,  T80:1| T84:1,  T80:2| T84:2,  T80:3| T84:3,  T81:0| T85:0,  T81:1| T85:1,  T81:2| T85:2,  T81:3| T85:3,  T82:0| T86:0,  T82:1| T86:1,  T82:2| T86:2,  T82:3| T86:3,  T83:0| T87:0,  T83:1| T87:1,  T83:2| T87:2,  T83:3| T87:3]
[  T88:0| T92:0,  T88:1| T92:1,  T88:2| T92:2,  T88:3| T92:3,  T89:0| T93:0,  T89:1| T93:1,  T89:2| T93:2,  T89:3| T93:3,  T90:0| T94:0,  T90:1| T94:1,  T90:2| T94:2,  T90:3| T94:3,  T91:0| T95:0,  T91:1| T95:1,  T91:2| T95:2,  T91:3| T95:3]
[  T96:0|T100:0,  T96:1|T100:1,  T96:2|T100:2,  T96:3|T100:3,  T97:0|T101:0,  T97:1|T101:1,  T97:2|T101:2,  T97:3|T101:3,  T98:0|T102:0,  T98:1|T102:1,  T98:2|T102:2,  T98:3|T102:3,  T99:0|T103:0,  T99:1|T103:1,  T99:2|T103:2,  T99:3|T103:3]
[ T104:0|T108:0, T104:1|T108:1, T104:2|T108:2, T104:3|T108:3, T105:0|T109:0, T105:1|T109:1, T105:2|T109:2, T105:3|T109:3, T106:0|T110:0, T106:1|T110:1, T106:2|T110:2, T106:3|T110:3, T107:0|T111:0, T107:1|T111:1, T107:2|T111:2, T107:3|T111:3]
[ T112:0|T116:0, T112:1|T116:1, T112:2|T116:2, T112:3|T116:3, T113:0|T117:0, T113:1|T117:1, T113:2|T117:2, T113:3|T117:3, T114:0|T118:0, T114:1|T118:1, T114:2|T118:2, T114:3|T118:3, T115:0|T119:0, T115:1|T119:1, T115:2|T119:2, T115:3|T119:3]
[ T120:0|T124:0, T120:1|T124:1, T120:2|T124:2, T120:3|T124:3, T121:0|T125:0, T121:1|T125:1, T121:2|T125:2, T121:3|T125:3, T122:0|T126:0, T122:1|T126:1, T122:2|T126:2, T122:3|T126:3, T123:0|T127:0, T123:1|T127:1, T123:2|T127:2, T123:3|T127:3]]
```

# pytorch-to-triton

pytorchä»£ç é€šè¿‡ `torch.compile` å¯ä»¥äº§ç”Ÿ triton kernelã€‚ `torch.compile` å¯ä»¥ä½¿ç”¨å¤šç§ backendï¼Œä¾‹å¦‚ `eager`ã€`inductor`(é»˜è®¤) ç­‰ã€‚

ä¸‹é¢æ˜¯ä¸€ä¸ªä» pytorch ä»£ç è½¬åˆ° triton kernel çš„ä¾‹ï¼Œåˆ©ç”¨äº† `triton.compile(backend="indunctor")`ã€‚

- è¾“å…¥: test_add.py

```python
import torch

def func(in1, in2):
    return torch.add(in1, in2)

opt_fn = torch.compile(func)

a = torch.randn(100, 10480, device="cuda")
b = torch.randn(100, 10480, device="cuda")
opt_fn(a,b)
```

- è¿è¡Œï¼š `python test_add.py`

åœ¨ `/tmp/torchinductor_${USER}` ä¸‹æ‰¾åˆ° triton code ä»¥åŠ ttir

```bash
/tmp/torchinductor_root# tree -L 3

.
|-- 5a # åŒ…å«__main__å°è£…çš„å‡½æ•°è°ƒç”¨è¡Œä¸º
|   `-- c5anvd6whgca2cpb7ukgizx5jlgxefgorn7ebtgxxtj2tm53rkom.py
|-- im # tuning æ—¶çš„æœ€ä¼˜config ä»¥åŠ ç”Ÿæˆçš„ triton kernel
|   |-- cimktqodcpl2shgn3s6k3t3aa7m4b3dogeudqxny3cvpjkrmjjtx.best_config
|   `-- cimktqodcpl2shgn3s6k3t3aa7m4b3dogeudqxny3cvpjkrmjjtx.py
`-- triton # compile cache
    `-- 0
        |-- 1801f53031fb6b10991a839c4c36f1ac09a877d0118cd6433bc1c4348b757831
        |-- 3a61b614a201d7c8de1da696f6276c6469631a07e1c1112872fcde349e60acb2
        |-- 83f4d801beeb824446d3ceb3e79e3e01bd3e2b91ab6fda189b715a72fcdf30e9
        |-- 9bebfd64edaf8e09d00770860d8e8ced2ff88b1f9ed6d7f2338124d13cc915c7
        |-- ae0c8a3e48147ccc432244a44fbd1d4793e58cd52158c26689f388047c187e97
        |-- b4c72811b23880731310c2e71420ebfb89924cf8f93373f68d0baa43ee03cf67
        `-- e6ab39b907a1a1838c7b1094681fe980548c7a9397404bba4b9daeb99b966266
```

å…¶ä¸­ç”Ÿæˆçš„ triton kernel (triton-lang) è¡¨ç¤ºä¸º

```python
import triton
import triton.language as tl
from triton.compiler.compiler import AttrsDescriptor

from torch._inductor import triton_helpers, triton_heuristics
from torch._inductor.ir import ReductionHint, TileHint
from torch._inductor.triton_helpers import libdevice, math as tl_math
from torch._inductor.triton_heuristics import AutotuneHint
from torch._inductor.utils import instance_descriptor

@triton_heuristics.pointwise(
    size_hints=[1048000],
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_0', 'mutated_arg_names': [], 'no_x_dim': False, 'backend_hash': '52964130331800C009CC77AD19A8AAFB702D8BB9B81D6B43FE21DEBE4B5B40E7'},
    min_elem_per_thread=0
)
@triton.jit
def triton_poi_fused_add_0(in_ptr0, in_ptr1, out_ptr0, xnumel, XBLOCK : tl.constexpr, XBLOCK_FRAGMENT : tl.constexpr):
    xnumel = 1048000
    xoffset_num = tl.cdiv(XBLOCK, XBLOCK_FRAGMENT)
    xstep = tl.num_programs(0) * XBLOCK_FRAGMENT
    xoffset_begin = tl.program_id(0) * XBLOCK_FRAGMENT
    for offset in range(xoffset_num):
        xoffset = offset % xoffset_num * xstep + xoffset_begin
        xindex = xoffset + tl.arange(0, XBLOCK_FRAGMENT)[:]
        xmask = xindex < xnumel
        x0 = xindex
        tmp0 = tl.load(in_ptr0 + (x0), xmask)
        tmp1 = tl.load(in_ptr1 + (x0), xmask)
        tmp2 = tmp0 + tmp1
        tl.store(out_ptr0 + (x0), tmp2, xmask)

```

è¿‡ç¨‹ç»†èŠ‚å¯ä»¥çœ‹ï¼š[triton_heuristics.py](https://github.com/pytorch/pytorch/blob/main/torch/_inductor/runtime/triton_heuristics.py)

æ³¨å†Œæ‰‹å†™çš„ triton kernelï¼šå¯ä»¥å‚è€ƒ flaggems åœ¨ Aten å±‚é¢ç›´æ¥å¯¹ç®—å­è¿›è¡Œè¦†ç›–é‡å†™ï¼Œä½†è¿™æ ·å°±ä¸æ˜¯èµ° Inductor äº†ã€‚è¿˜æ²¡äº†è§£å¦‚ä½•æ³¨å†Œåˆ° Inductorã€‚
