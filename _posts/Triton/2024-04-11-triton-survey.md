---
title: Triton Survey
author: tfruan
date: 2024-04-11 20:00:00 +0800
categories: [Triton]
tags: [Triton, Survey]
---

# background

## cuda vs triton

cudaå’Œtritonç¼–ç¨‹æ¨¡å¼å¯¹æ¯”

![cuda_vs_triton](/assets/img/blog/img_triton_survey/cuda_vs_triton.png)

ç›¸æ¯” cudaï¼Œ triton å…¶å®æ˜¯ **æ€§èƒ½å’Œæ•ˆç‡çš„ trade-off**ã€‚å¾ˆå¤š cuda ä¸Šäººä¸ºçš„æ“ä½œè¡Œä¸ºéƒ½æ˜¯äº¤ç»™ compiler æ¥è‡ªåŠ¨å®Œæˆï¼Œå¾ˆé€‚åˆå¯¹ç¡¬ä»¶äº†è§£è¾ƒå°‘çš„åŒå­¦æ¥å®ç°æ–°çš„æ¨¡å‹ç®—æ³•ã€‚

æ¯”CUDAçš„SIMTç¼–ç¨‹èŒƒå¼ï¼Œç”±å¤šä¸ªthreadå¹¶è¡Œå¤„ç†ï¼Œtritonæ›´æ¥è¿‘SIMDç¼–ç¨‹èŒƒå¼ï¼Œä¸€æ¬¡å¤„ç†ä¸€ç‰‡æ•°æ®ï¼ˆåŸºäºblockç®—æ³•çš„ç¼–ç¨‹èŒƒå¼ï¼‰

ç›´æ¥å¯¹çº¿ç¨‹å—è¿›è¡Œç¼–ç¨‹ï¼Œæ¯ä¸€ä¸ªæ“ä½œéƒ½æ˜¯åº”ç”¨åœ¨å—ä¸Šï¼Œä¸å†æ§åˆ¶å•ä¸ªçš„çº¿ç¨‹ï¼Œçœå»çº¿ç¨‹ä¹‹é—´çš„åŒæ­¥ç­‰æ“ä½œ

![cuda_triton](/assets/img/blog/img_triton_survey/cuda_triton.png)

## æ¨èrepo

OpenAI [Triton](https://github.com/triton-lang/triton/tree/main) æ˜¯ä»€ä¹ˆï¼Ÿè¿™ä¸ªé—®é¢˜å¾ˆå¤šå¤§ä½¬éƒ½å·²ç»å›ç­”è¿‡äº†ï¼Œé˜…è¯»å®Œä»¥ä¸‹ blog ç›¸ä¿¡å¤§å®¶ä¼šæœ‰ä¸ªåŸºç¡€çš„ç†è§£

- æ¨å†›è€å¸ˆçš„ [è°ˆè°ˆå¯¹OpenAI Tritonçš„ä¸€äº›ç†è§£](https://zhuanlan.zhihu.com/p/613244988)ï¼Œå¸®åŠ©å¤§å®¶å»ºç«‹ä¸€ä¸ªå®è§‚å°è±¡
- è‘£é‘«å¤§ä½¬çš„ [å¦‚ä½•å…¥é—¨ OpenAI Triton ç¼–ç¨‹?](https://www.zhihu.com/question/622685131/answer/3217107882)ï¼Œå¸®åŠ©äº†è§£æ›´å¤šå…³äºè¯­æ³•ä¸Šçš„ä¿¡æ¯
- BobHuangå¤§ä½¬çš„ [æµ…æ Triton æ‰§è¡Œæµç¨‹](https://zhuanlan.zhihu.com/p/712640431)ï¼Œå¸®åŠ©åˆå­¦è€…å¤§è‡´æ˜ç™½ä¸€æ®µ python ç¨‹åºå¦‚ä½•è¿›å…¥ triton pipelineï¼Œç„¶åè·‘èµ·æ¥ã€‚
- ç†è§£tritonè¯­æ³•çš„repoï¼š[triton-puzzles](https://github.com/srush/Triton-Puzzles)
- å¾ˆå¤šç”¨tritonå®ç°çš„kernelçš„repoï¼š[lightllm](https://github.com/ModelTC/lightllm)

# ç»„æˆ

triton çš„[ç»„æˆ](https://github.com/triton-lang/triton/tree/main/python/triton)ï¼š

- [language](https://github.com/triton-lang/triton/tree/main/python/triton/language)ï¼šå‰ç«¯çš„triton-langè¯­æ³•
- [compiler](https://github.com/triton-lang/triton/tree/main/python/triton/compiler)ï¼škernelç¼–è¯‘çš„è¡Œä¸ºï¼Œä¼šæ ¹æ®åç«¯è°ƒç”¨å¯¹åº”çš„pipelineï¼ŒæŠŠtriton-langä¸‹é™åˆ°ç¡¬ä»¶æ±‡ç¼–
- [runtime](https://github.com/triton-lang/triton/tree/main/python/triton/runtime)ï¼šcacheï¼Œauto-tuningï¼Œ jit ç­‰ç»„ä»¶
- [backends](https://github.com/triton-lang/triton/tree/main/python/triton/backends): ç¼–è¯‘å®Œåï¼Œåœ¨æ‰§è¡Œæ—¶æ‰ç”¨çš„æ˜¯çœŸå®åç«¯ï¼Œä¾‹å¦‚ [nvgpu](https://github.com/triton-lang/triton/tree/main/third_party/nvidia/backend)ï¼Œè¿™é‡Œä¸»è¦åŒ…å« compiler æ—¶çš„ pipeline ç»„ç»‡ï¼Œ launch å‡½æ•°(ä½¿ç”¨æ¨¡ç‰ˆåŠ çœŸå®kernelå‚æ•°ç”ŸæˆçœŸå®çš„launchå‡½æ•°)ç­‰

## languague

ä½œä¸º triton é¡¹ç›®çš„ fontendï¼ŒåŒ…å«æ¯ä¸ªåŸè¯­çš„æ„é€ è¡Œä¸ºï¼ˆä¾‹å¦‚opçš„æŸäº›ç±»å‹ä¼šå·å·è½¬ä¸ºé»˜è®¤çš„f32æ¥è®¡ç®—ï¼‰ã€‚

å®˜æ–¹æä¾›äº†åŸè¯­çš„ç”¨æ³•æ‰‹å†Œï¼š<https://triton-lang.org/main/index.html>

**tlè§„å®šäº†irå¿…é¡»è¡¨è¾¾ä¸ºstaticçš„**ï¼Œä¸å…è®¸ mlir ä¸­ dynamic shape çš„è¡¨è¾¾ã€‚

## compiler

ä»¥ nvgpu ä¸ºä¾‹ï¼Œtriton-lang çš„ä¸‹é™æµç¨‹ï¼š

triton-lang -> triton dialect -> triton gpu dialect -> nvgpu dialect -> llvm ir + nvvm ir -> ptx -> cnbin

![triton_arch_now](/assets/img/blog/img_triton_survey/triton_arch_now.png)

triton-lang(python) -> ast -> ttirï¼šéå†æ•´ä¸ªè¯­æ³•æ ‘ï¼Œé€šè¿‡code_generatorç›¸å…³ä»£ç ï¼Œå®šä¹‰visit_Assignç­‰å‡½æ•°ï¼Œé€šè¿‡mlir builderç”Ÿæˆå¯¹åº”çš„mlirã€‚

compileræ”¯æŒå¤šåç«¯çš„æ–¹å‘ï¼šé€šè¿‡Linalg dialect

![triton_arch](/assets/img/blog/img_triton_survey/triton_arch.png)

## runtime

### jit

`@triton.jit`è£…é¥°å™¨ è¡¨ç¤ºä¸‹é¢è¿™æ®µä»£ç æ˜¯ä¸€ä¸ª triton kernel

ç¼–è¯‘æµç¨‹ä¸­ï¼Œé¦–å…ˆä¼šå°† `jit` ä¿®é¥°çš„ kernel ç»™æ‰“åŒ…æˆä¸€ä¸ª `JITFunction`ï¼Œè§ [runtime/jit.py](https://github.com/triton-lang/triton/blob/main/python/triton/runtime/jit.py#L824)ã€‚

```python
def jit(
    fn: Optional[T] = None,
    *,
    version=None,
    repr: Optional[Callable] = None,
    launch_metadata: Optional[Callable] = None,
    do_not_specialize: Optional[Iterable[int]] = None,
    do_not_specialize_on_alignment: Optional[Iterable[int]] = None,
    debug: Optional[bool] = None,
    noinline: Optional[bool] = None,
) -> Union[JITFunction[T], Callable[[T], JITFunction[T]]]:
    def decorator(fn: T) -> JITFunction[T]:
        assert callable(fn)
        if os.getenv("TRITON_INTERPRET", "0") == "1":
            from .interpreter import InterpretedFunction
            return InterpretedFunction(fn) # ä½¿ç”¨Interpreteræ‰§è¡Œï¼Œå®Œå…¨ä¸ä¼šèµ°åˆ°ä»»ä½•ç¼–è¯‘æµç¨‹ï¼Œä½¿ç”¨numpy&torch api å°è£…
        else:
            return JITFunction(
                fn,
                version=version,
                do_not_specialize=do_not_specialize,
                debug=debug,
                noinline=noinline,
                repr=repr,
                launch_metadata=launch_metadata,
            ) # ç¼–è¯‘
```

> `@triton.jit` çš„ `do_not_specialize` å‚æ•°ï¼Œæ¥é˜»æ­¢ triton ç”Ÿæˆè¿‡å¤šçš„ kernelã€‚
> triton jit ä¼šä»¥æ¯ä¸€ä¸ªéæŒ‡é’ˆå‚æ•°ä¸ºå‡†ï¼Œå»ç”Ÿæˆä¸€ä¸ªkernelï¼Œæ¯”å¦‚æŸä¸€ä¸ªå‚æ•°è¿è¡Œæ—¶å–å€¼å¯èƒ½ä¸º1æˆ–0ï¼Œé‚£ä¹ˆ triton å°±ä¼šä¸ºå®ƒä»¬å„ç”Ÿæˆä¸€ä¸ªã€‚

`JITFunction` å¯¹è±¡ç»§æ‰¿è‡ª `KernelInterface`ï¼Œå¹¶å°è£…äº†ä¸€äº›è°ƒç”¨è¯­æ³•ç³–ã€‚

```python
# python/triton/runtime/jit.py
class KernelInterface(Generic[T]):
    run: T

    def __getitem__(self, grid) -> T:
        """
        A JIT function is launched with: fn[grid](*args, **kwargs).
        Hence JITFunction.__getitem__ returns a callable proxy that
        memorizes the grid.
        """
        return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)

class JITFunction(KernelInterface[T]):
    cache_hook = None
    ...
    def run(self, *args, grid, warmup, **kwargs):
        ...
        key = ''.join(sig_and_spec) + str((constexpr_vals, excess_kwargs))
        kernel = self.cache[device].get(key, None)

        if kernel is None:
            # Kernel is not cached; we have to compile.
            ...
            src = self.ASTSource(self, signature, constants, configs[0]) # ast è½¬ ttir
            kernel = self.compile( # ttir ä¸€è·¯ä¸‹é™åˆ° cnbin
                src,
                target=target,
                options=options.__dict__,
            )
            ...
        if not warmup:
            ...
            kernel.run(...) # è°ƒç”¨ drive apiæ‰§è¡Œcnbin
```

`lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)`ï¼šå°†gridå‚æ•°å°è£…ä¼ å…¥è¿›å», é™¤äº†å®šä¹‰çš„kernelå‚æ•°å¤–ï¼Œè¿˜ä¼šé¢å¤–ä¼ å…¥num_wrapsï¼Œ stages, streamç­‰å‚æ•°ã€‚

```python
# äººä¸ºå†™å‡ºçš„kernelè°ƒç”¨
add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=1024)

# çœŸå®çš„kernelè°ƒç”¨
add_kernel(x.data_ptr, y.data_ptr, output, n_elements, BLOCK_SIZE=1024, grid, num_warps=4, num_stages=3, extern_libs=None, stream=None, warmup=False)
```

### auto-tuning

@[auto-tuning](https://triton-lang.org/main/python-api/generated/triton.autotune.html) ï¼šç”± `@triton.jit`è£…é¥°çš„kernelå¯ä»¥è°ƒç”¨ `@auto-tuning` detectorè§¦å‘è‡ªåŠ¨è°ƒä¼˜

ä½¿ç”¨ä¸Šéœ€è¦æä¾›ä¸€ä¸ªconfigsï¼ˆåŒ…å«åœ¨kernelä¸­å®šä¹‰çš„ `tl.constexpr`ï¼‰åˆ—è¡¨ï¼Œautotuneä¼šå¤šæ¬¡è¿è¡Œkernelå‡½æ•°æ¥è¯„ä¼°configsä¸­çš„æ‰€æœ‰é…ç½®ã€‚ï¼ˆé…ç½®æ˜¯äººä¸ºç»™å‡ºçš„ï¼Œæ‰€ä»¥ç©ºé—´ä¸å¤§ï¼Œä¾èµ–äººä¸ºç»éªŒï¼‰

- keyï¼šå‚æ•°åˆ—è¡¨ï¼Œå½“keyä¸­çš„å‚æ•°æ”¹å˜æ—¶ï¼Œéœ€è¦é‡æ–°è¯„ä¼°configs

- prune_configs_byï¼šç”¨æˆ·å¯ä»¥ä¼ å…¥å‡½æ•°æ¥å¸®åŠ©å‡æï¼ˆä¾‹å¦‚åŸºäºæ€§èƒ½æ¨¡å‹çš„å‡½æ•°ï¼‰ï¼ŒåŠ å¿«æ”¶æ•›

- reset_to_zeroï¼šè¾“å…¥å‚æ•°ååˆ—è¡¨ï¼Œåœ¨è¿è¡Œå‰å°†è¿™äº›å‚æ•°é‡ç½®ä¸º0

- warmupï¼šæ¯ä¸ªconfigçš„warmupæ—¶é—´ï¼Œé»˜è®¤25ms

- repï¼šæ¯ä¸ªconfigçš„é‡å¤æ—¶é—´ï¼Œé»˜è®¤100ns

```python
@triton.autotune(
    configs=[
        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),
        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),
        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),
        triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),
    ],
    key=['M', 'N', 'K'],
)
```

å½“å‰**è¦æ±‚æ‰€æœ‰BLOCK_SIZEè®¾ç½®çš„å€¼éƒ½å¾—æ˜¯2æ¬¡å¹‚**ï¼Œå› ä¸ºåœ¨gpuä¸Šæ•°æ®ä¸º2æ¬¡å†ªçš„è§„æ¨¡æ—¶æ€§èƒ½æ›´å¥½ã€‚

```python
n_rows, n_cols = x.shape
BLOCK_SIZE = triton.next_power_of_2(n_cols)
```

### cache

> `triton cache` é»˜è®¤å­˜åœ¨ `~/.triton/cache/` æ–‡ä»¶å¤¹ä¸‹ï¼Œå½“ç„¶ä¹Ÿå¯ä»¥ä½¿ç”¨ `export TRITON_CACHE_DIR=xxx` æ¥æŒ‡å®šã€‚

triton æ˜¯ jit çš„æ‰§è¡Œæ¨¡å¼ï¼Œä½†ä¸ºäº†å‡å°‘ç¼–è¯‘æ—¶é—´ï¼Œå…¶å®ä¼šä¿ç•™æ¯æ¬¡ç¼–è¯‘å¾—åˆ°çš„ kernelï¼Œæ‰€ä»¥çœŸå®çš„ç¼–è¯‘æµç¨‹æ˜¯ï¼š

1.æ ¹æ® kernel ä¿¡æ¯ç”Ÿæˆä¸€ä¸ª `metadata_filename`(ä¸€ä¸ª json æ–‡ä»¶)ï¼Œç„¶ååœ¨cacheç›®å½•ä¸­æŸ¥æ‰¾

```python
    if not always_compile and metadata_path is not None:
        # cache hit!
        metadata = json.loads(Path(metadata_path).read_text())
        return CompiledKernel(src, metadata_group, hash) # hash
```

2.å¦‚æœæ‰¾åˆ°äº†å°±ç”¨ï¼Œæ²¡æ‰¾åˆ°å°±éœ€è¦ç¼–è¯‘

python->ast->ttir->...

3.æŸä¸ªopå¤šæ¬¡æ‰§è¡Œï¼Œä»€ä¹ˆæ—¶å€™ä¼šhit cacheï¼Œä»€ä¹ˆæ—¶å€™éœ€è¦é‡æ–°ç¼–è¯‘å‘¢ï¼Ÿ

è¿™éœ€è¦ä» triton ä½•æ—¶ä¼šäº§ç”Ÿä¸€ä¸ªæ–° cache è®²èµ·ã€‚ triton ä¼šä»¥ [key](https://github.com/triton-lang/triton/blob/main/python/triton/runtime/jit.py#L616) ä¸ºæ ¸å¿ƒï¼Œkey åŒ…å« `sig_and_spec`, `constexpr_vals` å’Œ `excess_kwargs`ã€‚

- `sig_and_spec`ï¼š å‡½æ•°å å’Œ å‚æ•°ç±»å‹ï¼Œç›´æ¥è¡¨ç°åœ¨ kernel ä¸Šã€‚å½“å‚æ•°ç±»å‹å¾ˆå¤šçš„æ—¶å€™ä¹Ÿå¯ä»¥ä½¿ç”¨ `do_not_specialize` æ¥æ’é™¤æ‰æŸäº›å‚æ•°çš„å½±å“ï¼Œæ¥é¿å…ç”Ÿæˆæ›´å¤šçš„ kernelã€‚
- `constexpr_vals`ï¼š æ ‡è®°ä¸º `tl.constexpr` çš„å‚æ•°
- `excess_kwargs`ï¼š`num_stages`, `num_warps`, `num_stages` ç­‰

## backend

### launch

ç”±äºæ¯ä¸ª kernel çš„å‚æ•°å¯èƒ½ä¸åŒï¼Œæ‰€ä»¥éœ€è¦ä¸ºå…¶ç”Ÿæˆä¸åŒçš„æ‰§è¡Œå‡½æ•°ï¼Œä¼šä½¿ç”¨ kernel çš„ä¸€äº›ä¿¡æ¯å’Œ[å›ºå®šçš„ä»£ç æ‹¼æ¥](https://github.com/triton-lang/triton/blob/main/third_party/nvidia/backend/driver.py#L147)ã€‚æœ€ç»ˆå˜æˆä¸€ä¸ªå¯ä»¥è°ƒä»¥è°ƒç”¨çš„æ¥å£ã€‚

# elements

è¿™é‡Œåªæ˜¯ç®€å•ä»‹ç»ä¸‹ï¼Œä¸¾ä¸ªğŸŒ°ï¼Œvector add

```python
import torch
import triton
import triton.language as tl

# è¿™æ˜¯kernel
@triton.jit
def add_kernel(x_ptr,  # *Pointer* to first input vector.
               y_ptr,  # *Pointer* to second input vector.
               output_ptr,  # *Pointer* to output vector.
               n_elements,  # Size of the vector.
               BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process.
               # NOTE: `constexpr` so it can be used as a shape value.
               ):
    # There are multiple 'programs' processing different data. We identify which program
    # we are here:
    pid = tl.program_id(axis=0)  # We use a 1D launch grid so axis is 0.
    # This program will process inputs that are offset from the initial data.
    # For instance, if you had a vector of length 256 and block_size of 64, the programs
    # would each access the elements [0:64, 64:128, 128:192, 192:256].
    # Note that offsets is a list of pointers:
    block_start = pid * BLOCK_SIZE
    offsets = block_start + tl.arange(0, BLOCK_SIZE)
    # Create a mask to guard memory operations against out-of-bounds accesses.
    mask = offsets < n_elements
    # Load x and y from DRAM, masking out any extra elements in case the input is not a
    # multiple of the block size.
    x = tl.load(x_ptr + offsets, mask=mask)
    y = tl.load(y_ptr + offsets, mask=mask)
    output = x + y
    # Write x + y back to DRAM.
    tl.store(output_ptr + offsets, output, mask=mask)

# è¿™æ˜¯launchå‡½æ•°
def add(x: torch.Tensor, y: torch.Tensor):
    # We need to preallocate the output.
    output = torch.empty_like(x)
    assert x.is_cuda and y.is_cuda and output.is_cuda
    n_elements = output.numel()
    # The SPMD launch grid denotes the number of kernel instances that run in parallel.
    # It is analogous to CUDA launch grids. It can be either Tuple[int], or Callable(metaparameters) -> Tuple[int].
    # In this case, we use a 1D grid where the size is the number of blocks:
    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']), )
    # NOTE:
    #  - Each torch.tensor object is implicitly converted into a pointer to its first element.
    #  - `triton.jit`'ed functions can be indexed with a launch grid to obtain a callable GPU kernel.
    #  - Don't forget to pass meta-parameters as keywords arguments.
    add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=1024)
    # We return a handle to z but, since `torch.cuda.synchronize()` hasn't been called, the kernel is still
    # running asynchronously at this point.
    return output
```

## input

- pointer

`x_ptr, y_ptr` æŒ‡é’ˆï¼Œä¸ºå…¶ä»£è¡¨çš„tensorçš„ç¬¬ä¸€ä¸ªå…ƒç´ çš„åœ°å€ã€‚ç”¨æ¥å°†æ•°æ®loadåˆ°memory

- hyper-parameter

è¶…å‚æ•° `tl.constexptr` ï¼Œè¿è¡Œæ—¶ä¼ å…¥çš„å€¼æ¥è‡ª compiler æœç´¢å¾—åˆ°ã€‚å¯¹äºä¸åŒçš„ç¡¬ä»¶ä½¿ç”¨æ—¶ï¼Œæœ€ä½³æ€§èƒ½ï¼ˆè®¿å­˜+è®¡ç®—ï¼‰çš„å‚æ•°å¯èƒ½æ˜¯ä¸åŒçš„ï¼Œåç»­ç”± Triton compiler æ¥è¿›è¡Œæœç´¢ä¸åŒçš„å€¼

- stride

è¾“å…¥ä¸­ä¸€èˆ¬ä¹Ÿæœ‰strideï¼Œå¯¹äºnç»´çš„tensor aï¼Œa.stride()ä¼šè¾“å‡ºä¸€ä¸ªnç»´æ•°ç»„ã€‚strideç”¨æ¥æ‰¾æ¯ä¸ªå…ƒç´ çš„æŒ‡é’ˆ

## pid

ä¸€ä¸ª job è¢«åˆ†æˆ grid ä¸ª taskã€‚`pid = tl.program_id(axis=0)` è¡¨ç¤ºå½“å‰æ˜¯ç¬¬å‡ ä¸ªtask

1. program_idæ˜¯è¿™ä¸ªè™šæ‹Ÿçš„**for å¾ªç¯ é‡Œé¢çš„ index** (ç¬¬å‡ æ¬¡å¾ªç¯ï¼Œå®é™…ä¸­è¿™äº›å¾ªç¯æ˜¯å¹¶è¡Œ)

```python
pid = tl.program_id(axis=0)
# å½“è®¿é—®æ•°æ®æ€»é•¿256, BLOCK_SIZE=64
# tl.arange(0, BLOCK_SIZE) -> [0, 63]
# 0ï¼Œ 64ï¼Œ 128ï¼Œ 192
block_start = pid * BLOCK_SIZE
# æ‰€ä»¥æ•°æ®è®¿é—®æ—¶æ˜¯æŒ‰ç…§ [0:64, 64:128, 128:192, 192:256]
offsets = block_start + tl.arange(0, BLOCK_SIZE)
```

2. `axis`Â , æ˜¯è¯´æ˜ "å¾ªç¯"æœ‰å‡ å±‚ï¼Œæ­¤å¤„ axis = 0è¡¨ç¤ºå±•å¼€ä¸º1ç»´æ¥è®¿é—®ï¼ˆç»´åº¦æ¦‚å¿µç±»æ¯”memrefçš„ç»´åº¦ï¼Œç¬¬ä¸€ç»´ç›¸å½“äºmemrefçš„æœ€å†…ç»´uï¼‰

axisæ˜¯å¯åŠ¨3d Gridçš„ç´¢å¼•ï¼Œå¿…é¡»æ˜¯0 / 1 / 2

## load & store

æ˜¾ç¤ºåœ°loadå’Œstoreï¼Œæ‰¹é‡æ•°æ®å¤„ç†ï¼Œä»¥BLOCKä¸ºå•ä½ï¼Œä¸€æ¬¡å¤„ç†ä¸€ä¸ªBLOCK_SIZEçš„æ•°æ®ï¼ŒSIMDè¡Œä¸º

loadï¼šä»DRAMè¯»åˆ°SRAMï¼›storeï¼šä»SRAMå†™å›DRAMï¼›å‡å°‘äº†DRAMä¸Šè®¡ç®—è¡Œä¸ºã€‚loadå’Œstoreåœ¨è¯­ä¹‰ä¸Šå¯ä»¥è§£é‡Šä¸ºgatherå’Œscatter

```python
# load å’Œ store æ—¶éƒ½æ˜¯ä½¿ç”¨åŸºåœ°å€åŠ åç§» è·å¾—ä¸€ç‰‡æ•°æ®ï¼Œmaskè¡¨ç¤ºåªè·å¾—è¿™ç‰‡æ•°æ®ä¸­çš„ä¸€éƒ¨åˆ†
x = tl.load(x_ptr + offsets, mask=mask)
y = tl.load(y_ptr + offsets, mask=mask)
# å†™å›æ—¶ä¹Ÿéœ€è¦mask
tl.store(output_ptr + offsets, output, mask=mask)
```

- offsets

offsets ä¸ºè¦å¤„ç†æ•°æ®çš„èŒƒå›´ï¼Œç”±å½“å‰block_startå’Œrangeè®¡ç®—è€Œæˆ

```python
block_start = pid * BLOCK_SIZE
offsets = block_start + tl.arange(0, BLOCK_SIZE)
```

- mask

mask ä¸ºé®ç›–ï¼Œç±»ä¼¼decoder Attnä¸­çš„maskã€‚ä¸€æ˜¯è§„èŒƒè®¿å­˜è¡Œä¸ºï¼Œé˜²æ­¢è¶Šç•Œï¼ˆæœ€åä¸€å—æ•°æ®å¤§å°å¯èƒ½ä¸æ»¡è¶³é˜²æ­¢ BLOCK_SIZE çš„å¤§å°ï¼‰ï¼›äºŒæ˜¯è¿‡æ»¤å¯¹æœ¬æ¬¡è®¡ç®—ä¸å¿…é¡»çš„æ•°æ®ã€‚

ä¾‹å¦‚offset=1024ï¼Œmaskä¸ºä¸€ä¸ª1024ç»´çš„æ•°ç»„ï¼Œæ¯ä¸ªæ•°ä¸º0/1ï¼Œå½“æŸä½ä¸º1æ—¶ï¼Œåˆ™loadè¯¥æ•°æ®ï¼Œå½“æŸä½ä¸º0æ—¶ï¼Œèˆå¼ƒã€‚

## grid

è°ƒç”¨kernelæ—¶ï¼Œéœ€è¦è¯´æ˜è¯¥kernelæ‰§è¡Œå¾ªç¯æœ‰å‡ å±‚ï¼Œæ¯å±‚æœ‰å‡ æ¬¡ï¼Œè¿™å°±æ˜¯ `grid` çš„æ¦‚å¿µ

ä»¥Matmulè€Œè¨€ï¼Œè‹¥Aä¸ºMxKï¼ŒBä¸ºKxNï¼Œé‚£ä¹ˆCçš„å¤§å°å°±æ˜¯MxNï¼ˆMå’ŒNä¸ºparallel axiså¤§å°ï¼ŒKä¸ºreductionè½´å¤§å°ï¼‰

æ¯æ¬¡åˆ†å—è®¡ç®—ï¼Œå•å—å¤§å°BLOCK_SIZE_M x BLOCK_SIZE_Nï¼Œæ€»å…±è¿›è¡Œ
$$
\frac{M}{\text{BLOCK\_{SIZE}\_{M}}} \times \frac{N}{\text{BLOCK\_{SIZE}\_{N}}}
$$
Tritonä¸­å…³äºgridå®šä¹‰ï¼š

```python
    grid = lambda META: (
        triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),
    )
    matmul_kernel[grid](
        a, b, c,
        M, N, K,
        a.stride(0), a.stride(1),
        b.stride(0), b.stride(1),
        c.stride(0), c.stride(1),
        ACTIVATION=activation
    )
```

å¯¹æ¯”Cudaä¸­launch kernelçš„è¡Œä¸º

```cpp
  dim3 block(BLOCK_SIZE_M, BLOCK_SIZE_N);
  dim3 grid((M + BLOCK_SIZE_M - 1) / BLOCK_SIZE_M, (N + BLOCK_SIZE_N - 1) / BLOCK_SIZE_N);
  matmul_kernel<<<grid,block>>>(Ad, Bd, Cd, M, N, K);
```

## num_warp

ä¸€èˆ¬ä½“ç°åœ¨module Atträ¸Šï¼Œä¸‹é¢çš„ä»£ç æ„å‘³ç€è¿™ä¸ªç¨‹åºä½¿ç”¨4ä¸ªwarpæ‰§è¡Œï¼ˆè¿™ä¸ªå‚æ•°ä¸€èˆ¬ä¹Ÿæ˜¯ `tl.constexpr`ï¼‰

```python
"triton_gpu.num-warps" = 4 : i32
```

tritongpu irç›¸æ¯”ttirä»…å¤šäº†ä¸€ä¸ªBlocked Layoutï¼Œæœ¬è´¨ä¸Šæè¿°çš„æ˜¯Blockå¯¹Memoryçš„Access Pattern

```python
 #blocked = #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>
```

å°±æ˜¯**ä¸€ä¸ªCTAé‡Œæœ‰4ä¸ªWarpï¼Œä¸€ä¸ªWarpæœ‰32ä¸ªThreadï¼Œä¸€ä¸ªThreadå¤„ç†1ä¸ªå…ƒç´ **ã€‚

Blocked Layoutåªæ˜¯ä¸€ç§Patternï¼Œä½†**æŒ‰ç…§è¿™ä¸ªPatternä¼šå¤šæ¬¡è®¿é—®ï¼Œæ€»è®¿é—®é‡è¾¾åˆ°BLOCK_SIZE**

## num_stages

compileråœ¨è½¯ä»¶æµæ°´æ—¶ä½¿ç”¨ã€‚è½¯ä»¶æµæ°´ä¼˜åŒ–ä¸€èˆ¬ä¼šåœ¨kernelä¸­æ’å…¥å¾ªç¯ï¼Œä»¥å®ç°å¯¹ä¸€ä¸ª `BLOCK_SIZE` çš„æ•°æ®è¿›è¡Œåˆ†æ®µè®¡ç®—

## layout

Layoutï¼šå®šä¹‰äº†Dataæ˜¯å¦‚ä½•è¢«Threadå¤„ç†ã€‚è¿™ç§layout attråœ¨loweringè¿‡ç¨‹ä¸­è¢«ä¼ é€’ï¼Œç”¨äºæè¿°opçš„æ‹†åˆ†æ˜ å°„å…³ç³»

> Block layoutç­‰ distributed layout æè¿°çº¿ç¨‹çš„è®¿å­˜è¡Œä¸ºï¼›shared layout æè¿°smemä¸­å“ªäº›å…ƒç´ ä¼šè¢«åŒæ—¶è®¿é—®ï¼Œç„¶åè¿›è¡Œswizzledï¼Œé˜²æ­¢banck conflict

- **Distributed Layoutï¼š**Blocked Layout, MMA Layout, DotOperand Layoutéƒ½å±äºæ­¤ç±»ã€‚è¿™äº›Layoutçš„ç‰¹ç‚¹éƒ½æ˜¯æ˜ å°„å‡½æ•°ä¼šå°†ç‰¹å®šçš„Tensoräº¤ç»™ç‰¹å®šçš„Threadå»å¤„ç†(å³ä¸€ä¸ªlayoutæè¿°æ•´ä¸ªtensorçš„è®¿é—®æ¨¡å¼)ï¼Œè¾¾åˆ°ä¸€ä¸ª**distribution**çš„æ•ˆæœ

```cpp
class DistributedEncoding<string name> : TritonGPU_Attr<name> {
  let description =[{
    The layout function \mathcal{L} of this layout is then defined, for an
    index `i` \in R^D, as follows:

    \mathcal{L}(A)[i_d] = L[(i_d + k_d*A.shape[d]) % L.shape[d]] \forall k_d such as i_d + k_d*A.shape[d] < L.shape[d]

    For example, for a tensor/layout pair
    A = [x  x  x  x  x  x  x  x]
        [x  x  x  x  x  x  x  x]
    L = [0  1  2  3 ]
        [4  5  6  7 ]
        [8  9  10 11]
        [12 13 14 15]

    Then the data of A would be distributed as follow between the 16 CUDA threads:
    // L(i, j) = {...} ç”¨æ¥æè¿°æ•°æ® (i, j) è¢«å“ªäº›CUDAçº¿ç¨‹è®¿é—®
    L(A) = [ {0,8} , {1,9} , {2,10}, {3,11}, {0,8} , {1, 9} , {2, 10}, {3, 11},
            {4,12}, {5,13}, {6,14}, {7,15}, {4,12}, {5, 13}, {6, 14}, {7, 15} ]
    }];
...
}
```

- **Shared Layout**: GPUä¸­çš„Shared Memoryæ˜¯å¯ä»¥è¢«ä¸€ä¸ªBlockå†…çš„ä»»æ„çº¿ç¨‹è®¿é—®çš„ï¼Œshared layoutä¼šè¢«ç”¨æ¥æè¿°å“ªäº›å…ƒç´ ä¼šè¢«çº¿ç¨‹åŒæ—¶è®¿é—®ï¼Œä»¥æ­¤æ¥å‡å°‘bank confictæ˜ å°„å‡½æ•°è¢«å®šä¹‰ä¸ºä»»æ„Tensor->ä»»æ„Threadã€‚

### distributed layout

Distributed encodings have a layout function that is entirely characterized by a d-dimensional tensor L. Note that L doesn't need to have the same shape (or even the same rank) as the tensor it is encoding.

æ˜ å°„å‡½æ•°(layout function)ä¼šå°†ç‰¹å®šçš„Tensoräº¤ç»™ç‰¹å®šçš„Threadå»å¤„ç†(å³ä¸€ä¸ªlayoutæè¿°æ•´ä¸ªtensorçš„è®¿é—®æ¨¡å¼)ï¼Œè¾¾åˆ°ä¸€ä¸ª**distribution**çš„æ•ˆæœ

![distribute_layout](/assets/img/blog/img_triton_survey/distribute_layout.png)

### block layout

æœ€å¸¸è§çš„ layoutï¼ŒåŒ…å«äº†é…åˆ AxisInfoAnalysis åˆ†æè·å¾— load å’Œ store çš„è®¿å­˜è¡Œä¸ºï¼Œä»¥ç”¨æ¥è®¿å­˜åˆå¹¶ã€‚

> ä¸€ä¸ª warp ä¸­çš„æ‰€æœ‰ thread åœ¨åŒä¸€æ—¶é—´ç‚¹åªèƒ½æ‰§è¡Œç›¸åŒçš„æŒ‡ä»¤ï¼Œæ‰€ä»¥éœ€è¦è®¿é—®çš„å†…å­˜è¶Šè¿ç»­ï¼Œæœ€å load/store transactions çš„æ•°é‡å°±è¶Šå°‘ã€‚é…åˆ shared layout æ¥è°ƒæ•´æ•°æ®åˆ†å¸ƒï¼Œå‡å°‘ transactionsã€‚

An encoding where each warp owns a contiguous portion of the target tensor. This is typically the kind of data layout **used to promote memory coalescing in LoadInst and StoreInst.**

`#blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 8], threadsPerWarp = [8, 4], warpsPerCTA = [8, 1], order = [1, 0]}>`

<img src="/assets/img/blog/img_triton_survey/cta_warp_thread.png" alt="Untitled" style="zoom:50%;" />

- sizePerThread = [1, 8]ï¼šæ¯ä¸ªçº¿ç¨‹å¤„ç†æ•°æ®Size
- threadsPerWarp = [8, 4]ï¼š warpå†…çº¿ç¨‹çš„å¸ƒå±€
- warpsPerCTA = [8, 1]ï¼šthread blockå†…warpçš„å¸ƒå±€
- order = [1, 0]ï¼šå…ˆè®¿é—®dim1ï¼Œå†è®¿é—®dim0

> Triton ä¼šä¼˜å…ˆ `Contiguity` æ›´å¤§çš„ç»´åº¦ï¼Œ `Contiguity`  ä¿¡æ¯ä¸€èˆ¬æ¥è‡ªäºä½¿ç”¨ `tl.max_contiguous(input, values)` äººä¸ºå‘ŠçŸ¥ç¼–è¯‘å™¨ï¼Œè¿™æ„å‘³ç€ input[i] ä¸­æ¯ values[i] ä¸ªç›¸é‚»å…ƒç´ æ˜¯è¿ç»­çš„ã€‚

è¯¥BLockè®¿å­˜æ¨¡å¼ä¸€æ¬¡èƒ½å¤„ç†(1x8x8, 8x4) = (64, 32)è§„æ¨¡çš„shapeã€‚ä½†è‹¥è¾“å…¥opçš„shapeä¸º(128, 32)ï¼Œé‚£ä¹ˆè®©æ¯ä¸ªthreadå¤„ç†ä¸¤ä¸ªè¿ç»­å—å³å¯ï¼Œå³ç¬¬ä¸€ä¸ªthreadå¤„ç†(0, 0:7), (64, 0:7)ä¸¤ä¸ªå—

å‡å¦‚ä¸€ä¸ª warp å¸Œæœ›è®¿é—® 128 ä¸ªæ•°ï¼Œ32 ä¸ª thread å¯ä»¥é€šè¿‡å››æ¬¡æ¬è¿å®Œæˆï¼š

```text
#blocked_before = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>
```

memory-coalesce åå°†ä¼šç”Ÿæˆä¸‹é¢çš„ Layout(ç¬¬äºŒç»´è¿ç»­æ›´é•¿ï¼Œæ‰€ä»¥orderä¹Ÿè¦è·Ÿç€æ”¹å˜)ï¼Œè¿™æ ·æ¯ä¸ª thread å¤„ç†çš„æ•°æ®æ›´å¤šï¼Œæ›´èƒ½åœ¨åç«¯æ˜ å°„æˆ vectorization æŒ‡ä»¤ã€‚

```text
#blocked_after = #triton_gpu.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>
```

### shared layout

In order to **avoid shared memoryÂ bank conflicts**, elements may beÂ **swizzled**Â in memory.

åŒä¸€ä¸ªwarpå†…çš„threadåŒæ—¶è®¿é—®åŒä¸€åˆ—çš„æ•°æ®ä¼šäº§ç”Ÿ bank å†²çªï¼Œå¯¹æ•°æ®è¿›è¡Œ swizzleï¼Œè°ƒæ•´ç›¸å…³çš„å­˜å‚¨ä½ç½®ï¼Œä¿è¯ thread è®¿é—®æ—¶ä¸å‡ºç° bank conflictã€‚

![swizzled memory](/assets/img/blog/img_triton_survey/swizzled.png)

### MMA Layout å’Œ DotOperand Layout

ç”¨æ¥æŒ‡å¯¼ op ä¸‹é™åˆ°ç‰¹æ®ŠæŒ‡ä»¤çš„ attrã€‚

# trick

## ä¸åŒçš„ç¯å¢ƒå˜é‡

- `MLIR_ENABLE_DUMP=1`

dumps the IR before every MLIR pass Triton runs

- `TRITON_PRINT_AUTOTUNING=1`

æ‰“å°æ¯æ¬¡é€‰æ‹©çš„ config

## æ‰“å°passå‰åir

- ä½¿ç”¨ `triton-opt` ç›´æ¥è·‘ pipelineï¼ŒåŠ ä¸Š `mlir-print-ir-after-all`
- æ”¹ python ä¸­ triton åº“ `site-packages/triton/backend/xxx/compiler.py` ä¸­çš„ä»£ç ï¼Œä¾‹å¦‚æ³¨é‡Šæ‰ä¸‹é¢æŸä¸ªpassæ¥çœ‹iræ˜¯å¦ä¸åŒ

```python
    # /usr/lib/python3.10/site-packages/triton/backends/xxx/compiler.py
    def make_ttir(mod, metadata, opt):
        pm = ir.pass_manager(mod.context)
        pm.enable_debug()
        passes.common.add_inliner(pm)
        passes.ttir.add_rewrite_tensor_pointer(pm)
        passes.ttir.add_combine(pm)
        passes.common.add_canonicalizer(pm)
        passes.ttir.add_reorder_broadcast(pm)
        passes.common.add_cse(pm)
        passes.common.add_licm(pm)
        passes.common.add_symbol_dce(pm)
        pm.run(mod)
        return mod
```
